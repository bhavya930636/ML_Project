{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127a1099-17be-41da-8047-eec08e23c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from utils import loadWord2Vec, clean_str\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a607a6-4231-4009-af7f-84af59a2cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_numbers = [110224, 146930, 2941, 104545, 62327, 29760, 96891, 47026, 117733, 163207, 61451, 20590, 145423, 33883, 4524, 81255, 82144, 85139, 167094, 125904, 116418, 158098, 95233, 81836, 84071, 53850, 112264, 17238, 34056, 10708, 164819, 32413, 75065, 139894, 161233, 100688, 148030, 55853, 23077, 152167, 44378, 111683, 145091, 132167, 83409, 153865, 143858, 111283, 150657, 36240, 142480, 112754, 149469, 50328, 163801, 45304, 155756, 151190, 160715, 8876, 116827, 142384, 143574, 136796, 278, 120989, 120991, 151642, 44083, 94917, 32815, 117614, 98217, 41937, 16626, 163885, 69261, 29037, 91370, 31452, 161506, 27949, 29236, 15855, 162541, 161900, 157800, 90014, 120276, 57310, 92875, 140990, 11693, 78368, 62286, 80503, 72673, 121791, 12663, 90678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5406d7f-94b7-4868-b5fa-1bf28a2a2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids=[]\n",
    "for line_no in line_numbers:\n",
    "    node_id = line_no - 1\n",
    "    node_ids.append(node_id)\n",
    "\n",
    "node_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4505bc-633b-4e26-86a6-3eed576b7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277, 2940, 4523, 8875, 10707]\n"
     ]
    }
   ],
   "source": [
    "print(node_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b0929f6-5d08-4f46-b737-4314440e839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['277\\ttrain\\t16', '2940\\ttrain\\t24', '4523\\ttrain\\t30', '8875\\ttrain\\t24', '10707\\ttrain\\t28', '11692\\ttrain\\t4', '12662\\ttrain\\t24', '15854\\ttrain\\t8', '16625\\ttrain\\t16', '17237\\ttrain\\t16', '20589\\ttrain\\t27', '23076\\ttrain\\t28', '27948\\ttrain\\t10', '29036\\ttrain\\t28', '29235\\ttrain\\t5', '29759\\ttrain\\t4', '31451\\ttrain\\t23', '32412\\ttrain\\t39', '32814\\ttrain\\t2', '33882\\ttrain\\t2', '34055\\ttrain\\t34', '36239\\ttrain\\t25', '41936\\ttrain\\t20', '44082\\ttrain\\t32', '44377\\ttrain\\t16', '45303\\ttrain\\t4', '47025\\ttrain\\t34', '50327\\ttrain\\t24', '53849\\ttrain\\t28', '55852\\ttrain\\t24', '57309\\ttrain\\t31', '61450\\ttrain\\t30', '62285\\ttrain\\t25', '62326\\ttrain\\t28', '69260\\ttrain\\t19', '72672\\ttrain\\t22', '75064\\ttrain\\t16', '78367\\ttrain\\t26', '80502\\ttrain\\t24', '81254\\ttrain\\t27', '81835\\ttrain\\t11', '82143\\ttrain\\t28', '83408\\ttrain\\t28', '84070\\ttrain\\t16', '85138\\ttrain\\t19', '90013\\ttrain\\t34', '90677\\ttrain\\t4', '91369\\ttrain\\t31', '92874\\ttrain\\t4', '94916\\ttrain\\t24', '95232\\ttrain\\t24', '96890\\ttrain\\t5', '98216\\ttrain\\t39', '100687\\ttrain\\t18', '104544\\ttrain\\t28', '110223\\ttrain\\t10', '111282\\ttrain\\t16', '111682\\ttrain\\t39', '112263\\ttrain\\t13', '112753\\ttrain\\t28', '116417\\ttrain\\t16', '116826\\ttrain\\t3', '117613\\ttrain\\t16', '117732\\ttrain\\t24', '120275\\ttrain\\t31', '120988\\ttrain\\t16', '120990\\ttrain\\t28', '121790\\ttrain\\t15', '125903\\ttrain\\t20', '132166\\ttrain\\t27', '136795\\ttrain\\t16', '139893\\ttrain\\t8', '140989\\ttrain\\t24', '142383\\ttrain\\t4', '142479\\ttrain\\t33', '143573\\ttrain\\t31', '143857\\ttrain\\t24', '145090\\ttrain\\t8', '145422\\ttrain\\t24', '146929\\ttrain\\t27', '148029\\ttrain\\t16', '149468\\ttrain\\t30', '150656\\ttrain\\t5', '151189\\ttrain\\t28', '151641\\ttrain\\t16', '152166\\ttrain\\t26', '153864\\ttrain\\t13', '155755\\ttrain\\t28', '157799\\ttrain\\t7', '158097\\ttrain\\t16', '160714\\ttrain\\t39', '161232\\ttrain\\t16', '161505\\ttrain\\t5', '161899\\ttrain\\t28', '162540\\ttrain\\t19', '163206\\ttrain\\t24', '163800\\ttrain\\t24', '163884\\ttrain\\t16', '164818\\ttrain\\t16', '167093\\ttrain\\t28']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/graph_node_labels.txt', 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "# Process only the lines with the given indices\n",
    "for i, line in enumerate(lines):\n",
    "    if i in node_ids:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "f.close()\n",
    "print(doc_train_list)\n",
    "print(doc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a187027-4bc8-4f00-bc80-727d972233d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['construction q constant weight sequences using like approach present scheme constant weight sequences , , given information sequence , construction results sequence specific weight within certain scheme uses design based codes , weight sequences , applications', 'decision study agent population identify supported proposal change , space deliberation process agents , two new proposal , agents identify class spaces , spaces , deliberation process result success deliberation , long supported proposal consider general proposal space deliberation setting study conditions deliberation success analysis dynamic algorithm identify supported proposal', 'decentralized dynamic optimization power network voltage control voltage control power distribution networks devices devices also provide limited power resources used network wide voltage decentralized voltage control strategy voltage mismatch error using gradient \\\\( \\\\) power network , local voltage provide gradient information paper aims analyze performance decentralized based voltage control design two dynamic scenarios \\\\) nodes perform decentralized , \\\\) network time varying voltage control , improve existing convergence voltage based gradient modeling network using process time varying constraints , provide error bound tracking optimal solution error result extended general dynamic optimization problems stochastic processes bounded changes numerical demonstrate results realistic power networks', 'reduction paper , propose procedure given \\\\( \\\\) , language deterministic one b size size bound number , used , obtained', 'rate lower bounds distributed storage one distributed storage system large amount source data long using large number n storage nodes , capacity storage system capacity available , e , 1 n storage nodes time nodes , thus data system average rate n , 1 average node source data , data network nodes average rate , data nodes based data main result , , source data point time must case 2 n provides lower bound average rate data system order source data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/clean_100.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "print(doc_content_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f22516-f429-4c96-86b6-95e1ce3a2891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[]\n",
      "[11, 52, 58, 82, 97, 53, 23, 88, 65, 67, 34, 92, 66, 22, 8, 59, 91, 19, 45, 41, 50, 57, 49, 95, 69, 12, 24, 21, 54, 63, 36, 80, 78, 1, 27, 75, 90, 44, 35, 18, 20, 74, 55, 46, 83, 0, 40, 9, 14, 30, 99, 48, 72, 37, 7, 71, 85, 73, 56, 70, 43, 25, 33, 5, 94, 86, 2, 13, 28, 42, 84, 62, 26, 10, 96, 61, 60, 51, 77, 76, 68, 98, 39, 87, 29, 79, 38, 17, 4, 64, 3, 89, 31, 93, 16, 6, 32, 81, 47, 15]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "doc_content_list = []\n",
    "f = open('data/corpus/clean_100.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('data/train_100.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('data/test_100.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "print(ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b9f9c-3cef-465f-bb13-e3b1037a431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('data/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_name_str)\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()\n",
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'w')\n",
    "f.write(vocab_str)\n",
    "f.close()\n",
    "\n",
    "'''\n",
    "Word definitions begin\n",
    "'''\n",
    "'''\n",
    "definitions = []\n",
    "\n",
    "for word in vocab:\n",
    "    word = word.strip()\n",
    "    synsets = wn.synsets(clean_str(word))\n",
    "    word_defs = []\n",
    "    for synset in synsets:\n",
    "        syn_def = synset.definition()\n",
    "        word_defs.append(syn_def)\n",
    "    word_des = ' '.join(word_defs)\n",
    "    if word_des == '':\n",
    "        word_des = '<PAD>'\n",
    "    definitions.append(word_des)\n",
    "\n",
    "string = '\\n'.join(definitions)\n",
    "\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\n",
    "f.write(string)\n",
    "f.close()\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vec.fit_transform(definitions)\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\n",
    "\n",
    "word_vectors = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    vector = tfidf_matrix_array[i]\n",
    "    str_vector = []\n",
    "    for j in range(len(vector)):\n",
    "        str_vector.append(str(vector[j]))\n",
    "    temp = ' '.join(str_vector)\n",
    "    word_vector = word + ' ' + temp\n",
    "    word_vectors.append(word_vector)\n",
    "\n",
    "string = '\\n'.join(word_vectors)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\n",
    "f.write(string)\n",
    "f.close()\n",
    "\n",
    "word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "word_embeddings_dim = len(embd[0])\n",
    "'''\n",
    "\n",
    "'''\n",
    "Word definitions end\n",
    "'''\n",
    "\n",
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "f = open('data/corpus/' + dataset + '_labels.txt', 'w')\n",
    "f.write(label_list_str)\n",
    "f.close()\n",
    "\n",
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "# different training rates\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "f = open('data/' + dataset + '.real_train.name', 'w')\n",
    "f.write(real_train_doc_names_str)\n",
    "f.close()\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "print(ty)\n",
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# word vector cosine similarity as weights\n",
    "\n",
    "'''\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
    "            vector_i = np.array(word_vector_map[vocab[i]])\n",
    "            vector_j = np.array(word_vector_map[vocab[j]])\n",
    "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
    "            if similarity > 0.9:\n",
    "                print(vocab[i], vocab[j], similarity)\n",
    "                row.append(train_size + i)\n",
    "                col.append(train_size + j)\n",
    "                weight.append(similarity)\n",
    "'''\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "f = open(\"data/ind.{}.x\".format(dataset), 'wb')\n",
    "pkl.dump(x, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.y\".format(dataset), 'wb')\n",
    "pkl.dump(y, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.tx\".format(dataset), 'wb')\n",
    "pkl.dump(tx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.ty\".format(dataset), 'wb')\n",
    "pkl.dump(ty, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.allx\".format(dataset), 'wb')\n",
    "pkl.dump(allx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.ally\".format(dataset), 'wb')\n",
    "pkl.dump(ally, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.adj\".format(dataset), 'wb')\n",
    "pkl.dump(adj, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
