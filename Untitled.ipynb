{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672a6b12-48cf-4a65-8c54-2f83d19876d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 19:22:01.798482: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 19:22:02.014981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 19:22:02.119430: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 19:22:02.146855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 19:22:02.314181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 19:22:03.450689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCN, MLP\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/sem5/ML/Project/ML_Project/models.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/sem5/ML/Project/ML_Project/layers.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m flags \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39mflags\n\u001b[1;32m      5\u001b[0m FLAGS \u001b[38;5;241m=\u001b[39m flags\u001b[38;5;241m.\u001b[39mFLAGS\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# global unique layer ID dictionary for layer name assignment\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'app'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c97b0e-cf41-4915-b69b-bfa2845cfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed\n",
    "seed = random.randint(1, 200)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "# 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('dataset', dataset, 'Dataset string.')\n",
    "# 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
    "flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0,\n",
    "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
    "flags.DEFINE_integer('early_stopping', 10,\n",
    "                     'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n",
    "    FLAGS.dataset)\n",
    "print(adj)\n",
    "# print(adj[0], adj[1])\n",
    "features = sp.identity(features.shape[0])  # featureless\n",
    "\n",
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    # helper variable for sparse dropout\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "print(features[2][1])\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(\n",
    "        features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(\n",
    "        features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
    "                     model.layers[0].embedding], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, pred, labels, duration = evaluate(\n",
    "        features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(\n",
    "              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, pred, labels, test_duration = evaluate(\n",
    "    features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "print(len(test_mask))\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(\"Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
    "\n",
    "# doc and word embeddings\n",
    "print('embeddings:')\n",
    "word_embeddings = outs[3][train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = outs[3][:train_size]  # include val docs\n",
    "test_doc_embeddings = outs[3][adj.shape[0] - test_size:]\n",
    "\n",
    "print(len(word_embeddings), len(train_doc_embeddings),\n",
    "      len(test_doc_embeddings))\n",
    "print(word_embeddings)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'r')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "vocab_size = len(words)\n",
    "word_vectors = []\n",
    "for i in range(vocab_size):\n",
    "    word = words[i].strip()\n",
    "    word_vector = word_embeddings[i]\n",
    "    word_vector_str = ' '.join([str(x) for x in word_vector])\n",
    "    word_vectors.append(word + ' ' + word_vector_str)\n",
    "\n",
    "word_embeddings_str = '\\n'.join(word_vectors)\n",
    "f = open('data/' + dataset + '_word_vectors.txt', 'w')\n",
    "f.write(word_embeddings_str)\n",
    "f.close()\n",
    "\n",
    "doc_vectors = []\n",
    "doc_id = 0\n",
    "for i in range(train_size):\n",
    "    doc_vector = train_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "for i in range(test_size):\n",
    "    doc_vector = test_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
    "f = open('data/' + dataset + '_doc_vectors.txt', 'w')\n",
    "f.write(doc_embeddings_str)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
