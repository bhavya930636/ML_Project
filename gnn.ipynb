{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e647c609-3872-4cf9-8453-c031298b9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "from logger import Logger\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, node_list):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create a mask for the specific nodes\n",
    "    mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    mask[node_list] = True  # Set the specific nodes to True\n",
    "\n",
    "    # Ensure split indices are tensors\n",
    "    train_idx = torch.tensor(split_idx['train'], dtype=torch.long)\n",
    "    valid_idx = torch.tensor(split_idx['valid'], dtype=torch.long)\n",
    "    test_idx = torch.tensor(split_idx['test'], dtype=torch.long)\n",
    "\n",
    "    # Use the mask to filter indices correctly for each split\n",
    "    train_mask = mask[train_idx]\n",
    "    valid_mask = mask[valid_idx]\n",
    "    test_mask = mask[test_idx]\n",
    "\n",
    "    # Filter the true and predicted labels\n",
    "    y_true_train = data.y[train_idx][train_mask]\n",
    "    y_pred_train = y_pred[train_idx][train_mask]\n",
    "\n",
    "    y_true_valid = data.y[valid_idx][valid_mask]\n",
    "    y_pred_valid = y_pred[valid_idx][valid_mask]\n",
    "\n",
    "    y_true_test = data.y[test_idx][test_mask]\n",
    "    y_pred_test = y_pred[test_idx][test_mask]\n",
    "\n",
    "    # Debugging outputs\n",
    "    print(f\"Train: {y_true_train.numel()}, {y_pred_train.numel()}\")\n",
    "    print(f\"Valid: {y_true_valid.numel()}, {y_pred_valid.numel()}\")\n",
    "    print(f\"Test: {y_true_test.numel()}, {y_pred_test.numel()}\")\n",
    "\n",
    "    # Evaluate using the filtered true and predicted labels\n",
    "    train_acc = evaluator.eval({'y_true': y_true_train, 'y_pred': y_pred_train})['acc'] if y_true_train.numel() > 0 else 0.0\n",
    "    valid_acc = evaluator.eval({'y_true': y_true_valid, 'y_pred': y_pred_valid})['acc'] if y_true_valid.numel() > 0 else 0.0\n",
    "    test_acc = evaluator.eval({'y_true': y_true_test, 'y_pred': y_pred_test})['acc'] if y_true_test.numel() > 0 else 0.0\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae993947-5063-4250-86a2-b52c3a03b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.main.<locals>.Args object at 0x7f4aef0d7160>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kriti-arora/.local/lib/python3.8/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   277,   2940,  10707,  11692,  12662,  27948,  29235,  31451,  32814,\n",
      "         33882,  34055,  36239,  41936,  44082,  45303,  47025,  55852,  57309,\n",
      "         61450,  69260,  72672,  78367,  82143,  83408,  85138,  91369,  92874,\n",
      "         96890,  98216, 100687, 104544, 110223, 111282, 112753, 116826, 120990,\n",
      "        121790, 125903, 139893, 142383, 142479, 145090, 148029, 151189, 152166,\n",
      "        153864, 155755, 157799, 161505, 161899, 162540, 163206, 167093])\n",
      "tensor([  4523,  16625,  23076,  32412,  44377,  80502,  81254,  90677, 111682,\n",
      "        120988, 136795, 143573, 143857, 158097, 160714, 163800])\n",
      "tensor([  8875,  15854,  17237,  20589,  29036,  29759,  50327,  53849,  62285,\n",
      "         62326,  75064,  81835,  84070,  90013,  94916,  95232, 112263, 116417,\n",
      "        117613, 117732, 120275, 132166, 140989, 145422, 146929, 149468, 150656,\n",
      "        151641, 161232, 163884, 164818])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32816/2671334735.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_idx = torch.tensor(split_idx['train'], dtype=torch.long)\n",
      "/tmp/ipykernel_32816/2671334735.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_idx = torch.tensor(split_idx['valid'], dtype=torch.long)\n",
      "/tmp/ipykernel_32816/2671334735.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_idx = torch.tensor(split_idx['test'], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 01, Loss: 4.2909, Train: 9.43%, Valid: 6.25%, Test: 3.23%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 02, Loss: 1.8086, Train: 20.75%, Valid: 6.25%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 03, Loss: 0.9229, Train: 32.08%, Valid: 25.00%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 04, Loss: 0.5663, Train: 33.96%, Valid: 25.00%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 05, Loss: 0.3799, Train: 41.51%, Valid: 31.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 06, Loss: 0.2455, Train: 49.06%, Valid: 25.00%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 07, Loss: 0.2215, Train: 54.72%, Valid: 25.00%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 08, Loss: 0.1425, Train: 64.15%, Valid: 18.75%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 09, Loss: 0.0797, Train: 71.70%, Valid: 12.50%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 10, Loss: 0.0617, Train: 75.47%, Valid: 6.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 11, Loss: 0.0712, Train: 77.36%, Valid: 6.25%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 12, Loss: 0.0518, Train: 77.36%, Valid: 12.50%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 13, Loss: 0.0448, Train: 79.25%, Valid: 18.75%, Test: 45.16%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 14, Loss: 0.0321, Train: 84.91%, Valid: 25.00%, Test: 48.39%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 15, Loss: 0.0176, Train: 84.91%, Valid: 25.00%, Test: 51.61%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 16, Loss: 0.0167, Train: 88.68%, Valid: 25.00%, Test: 54.84%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 17, Loss: 0.0167, Train: 90.57%, Valid: 31.25%, Test: 51.61%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 18, Loss: 0.0169, Train: 90.57%, Valid: 31.25%, Test: 48.39%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 19, Loss: 0.0074, Train: 92.45%, Valid: 25.00%, Test: 48.39%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 01, Epoch: 20, Loss: 0.0094, Train: 92.45%, Valid: 25.00%, Test: 45.16%\n",
      "Run 01:\n",
      "Highest Train: 92.45\n",
      "Highest Valid: 31.25\n",
      "  Final Train: 41.51\n",
      "   Final Test: 29.03\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 01, Loss: 4.1059, Train: 9.43%, Valid: 6.25%, Test: 6.45%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 02, Loss: 1.5346, Train: 22.64%, Valid: 6.25%, Test: 9.68%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 03, Loss: 0.8337, Train: 28.30%, Valid: 25.00%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 04, Loss: 0.5037, Train: 47.17%, Valid: 25.00%, Test: 16.13%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 05, Loss: 0.3048, Train: 58.49%, Valid: 31.25%, Test: 19.35%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 06, Loss: 0.2565, Train: 47.17%, Valid: 25.00%, Test: 22.58%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 07, Loss: 0.1677, Train: 49.06%, Valid: 18.75%, Test: 25.81%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 08, Loss: 0.1280, Train: 56.60%, Valid: 18.75%, Test: 22.58%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 09, Loss: 0.0939, Train: 69.81%, Valid: 12.50%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 10, Loss: 0.0813, Train: 73.58%, Valid: 12.50%, Test: 25.81%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 11, Loss: 0.0398, Train: 79.25%, Valid: 12.50%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 12, Loss: 0.0438, Train: 81.13%, Valid: 12.50%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 13, Loss: 0.0354, Train: 84.91%, Valid: 25.00%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 14, Loss: 0.0360, Train: 88.68%, Valid: 37.50%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 15, Loss: 0.0257, Train: 90.57%, Valid: 37.50%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 16, Loss: 0.0287, Train: 90.57%, Valid: 31.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 17, Loss: 0.0133, Train: 88.68%, Valid: 31.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 18, Loss: 0.0129, Train: 88.68%, Valid: 31.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 19, Loss: 0.0217, Train: 90.57%, Valid: 31.25%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 02, Epoch: 20, Loss: 0.0620, Train: 90.57%, Valid: 31.25%, Test: 35.48%\n",
      "Run 02:\n",
      "Highest Train: 90.57\n",
      "Highest Valid: 37.50\n",
      "  Final Train: 88.68\n",
      "   Final Test: 35.48\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 01, Loss: 3.9342, Train: 20.75%, Valid: 6.25%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 02, Loss: 1.5947, Train: 30.19%, Valid: 18.75%, Test: 16.13%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 03, Loss: 0.8503, Train: 39.62%, Valid: 25.00%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 04, Loss: 0.5094, Train: 47.17%, Valid: 12.50%, Test: 3.23%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 05, Loss: 0.3247, Train: 47.17%, Valid: 12.50%, Test: 3.23%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 06, Loss: 0.2512, Train: 47.17%, Valid: 12.50%, Test: 3.23%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 07, Loss: 0.1732, Train: 52.83%, Valid: 6.25%, Test: 3.23%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 08, Loss: 0.1413, Train: 49.06%, Valid: 0.00%, Test: 3.23%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 09, Loss: 0.0927, Train: 45.28%, Valid: 0.00%, Test: 0.00%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 10, Loss: 0.0712, Train: 45.28%, Valid: 0.00%, Test: 0.00%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 11, Loss: 0.0523, Train: 47.17%, Valid: 0.00%, Test: 0.00%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 12, Loss: 0.0530, Train: 52.83%, Valid: 0.00%, Test: 6.45%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 13, Loss: 0.0453, Train: 54.72%, Valid: 0.00%, Test: 6.45%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 14, Loss: 0.0325, Train: 60.38%, Valid: 6.25%, Test: 9.68%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 15, Loss: 0.0268, Train: 77.36%, Valid: 6.25%, Test: 25.81%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 16, Loss: 0.0179, Train: 84.91%, Valid: 6.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 17, Loss: 0.0205, Train: 86.79%, Valid: 12.50%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 18, Loss: 0.0125, Train: 92.45%, Valid: 12.50%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 19, Loss: 0.0082, Train: 94.34%, Valid: 18.75%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 03, Epoch: 20, Loss: 0.0092, Train: 98.11%, Valid: 18.75%, Test: 29.03%\n",
      "Run 03:\n",
      "Highest Train: 98.11\n",
      "Highest Valid: 25.00\n",
      "  Final Train: 39.62\n",
      "   Final Test: 32.26\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 01, Loss: 4.1515, Train: 16.98%, Valid: 6.25%, Test: 6.45%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 02, Loss: 1.5960, Train: 24.53%, Valid: 12.50%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 03, Loss: 0.9517, Train: 30.19%, Valid: 25.00%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 04, Loss: 0.5738, Train: 47.17%, Valid: 18.75%, Test: 41.94%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 05, Loss: 0.3803, Train: 66.04%, Valid: 25.00%, Test: 41.94%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 06, Loss: 0.2716, Train: 71.70%, Valid: 25.00%, Test: 48.39%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 07, Loss: 0.1838, Train: 77.36%, Valid: 25.00%, Test: 41.94%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 08, Loss: 0.1273, Train: 77.36%, Valid: 18.75%, Test: 38.71%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 09, Loss: 0.1171, Train: 84.91%, Valid: 12.50%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 10, Loss: 0.0726, Train: 83.02%, Valid: 6.25%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 11, Loss: 0.0604, Train: 90.57%, Valid: 18.75%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 12, Loss: 0.0410, Train: 88.68%, Valid: 18.75%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 13, Loss: 0.0475, Train: 86.79%, Valid: 25.00%, Test: 29.03%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 14, Loss: 0.0306, Train: 84.91%, Valid: 31.25%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 15, Loss: 0.0343, Train: 84.91%, Valid: 31.25%, Test: 32.26%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 16, Loss: 0.0222, Train: 84.91%, Valid: 31.25%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 17, Loss: 0.0246, Train: 83.02%, Valid: 31.25%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 18, Loss: 0.0219, Train: 84.91%, Valid: 31.25%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 19, Loss: 0.0100, Train: 88.68%, Valid: 31.25%, Test: 35.48%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 04, Epoch: 20, Loss: 0.0088, Train: 88.68%, Valid: 31.25%, Test: 35.48%\n",
      "Run 04:\n",
      "Highest Train: 90.57\n",
      "Highest Valid: 31.25\n",
      "  Final Train: 84.91\n",
      "   Final Test: 32.26\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 01, Loss: 4.2002, Train: 13.21%, Valid: 6.25%, Test: 6.45%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 02, Loss: 1.7169, Train: 20.75%, Valid: 6.25%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 03, Loss: 0.8994, Train: 26.42%, Valid: 12.50%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 04, Loss: 0.5238, Train: 37.74%, Valid: 12.50%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 05, Loss: 0.3634, Train: 52.83%, Valid: 18.75%, Test: 9.68%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 06, Loss: 0.2268, Train: 62.26%, Valid: 25.00%, Test: 12.90%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 07, Loss: 0.1615, Train: 75.47%, Valid: 18.75%, Test: 16.13%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 08, Loss: 0.1583, Train: 79.25%, Valid: 12.50%, Test: 19.35%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 09, Loss: 0.1209, Train: 84.91%, Valid: 12.50%, Test: 38.71%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 10, Loss: 0.0909, Train: 86.79%, Valid: 18.75%, Test: 38.71%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 11, Loss: 0.0454, Train: 88.68%, Valid: 37.50%, Test: 38.71%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 12, Loss: 0.0477, Train: 90.57%, Valid: 37.50%, Test: 41.94%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 13, Loss: 0.0399, Train: 92.45%, Valid: 43.75%, Test: 41.94%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 14, Loss: 0.0260, Train: 92.45%, Valid: 31.25%, Test: 45.16%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 15, Loss: 0.0236, Train: 92.45%, Valid: 31.25%, Test: 45.16%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 16, Loss: 0.0208, Train: 92.45%, Valid: 37.50%, Test: 45.16%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 17, Loss: 0.0183, Train: 96.23%, Valid: 37.50%, Test: 45.16%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 18, Loss: 0.0083, Train: 96.23%, Valid: 37.50%, Test: 38.71%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 19, Loss: 0.0121, Train: 96.23%, Valid: 37.50%, Test: 38.71%\n",
      "Train: 53, 53\n",
      "Valid: 16, 16\n",
      "Test: 31, 31\n",
      "Run: 05, Epoch: 20, Loss: 0.0086, Train: 96.23%, Valid: 37.50%, Test: 38.71%\n",
      "Run 05:\n",
      "Highest Train: 96.23\n",
      "Highest Valid: 43.75\n",
      "  Final Train: 92.45\n",
      "   Final Test: 41.94\n",
      "All runs:\n",
      "Highest Train: 93.58 ± 3.43\n",
      "Highest Valid: 33.75 ± 7.13\n",
      "  Final Train: 69.43 ± 26.50\n",
      "   Final Test: 34.19 ± 4.89\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Detect if running in Jupyter and simulate argument parsing\n",
    "    if \"ipykernel\" in sys.argv[0]:\n",
    "        class Args:\n",
    "            device = 0\n",
    "            log_steps = 1\n",
    "            use_sage = False\n",
    "            num_layers = 3\n",
    "            hidden_channels = 256\n",
    "            dropout = 0.5\n",
    "            lr = 0.01\n",
    "            epochs = 20\n",
    "            runs = 5\n",
    "        args = Args()\n",
    "    else:\n",
    "        # Argument parsing for command-line execution\n",
    "        parser = argparse.ArgumentParser(description='OGBN-Arxiv (GNN)')\n",
    "        parser.add_argument('--device', type=int, default=0)\n",
    "        parser.add_argument('--log_steps', type=int, default=1)\n",
    "        parser.add_argument('--use_sage', action='store_true')\n",
    "        parser.add_argument('--num_layers', type=int, default=3)\n",
    "        parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "        parser.add_argument('--dropout', type=float, default=0.5)\n",
    "        parser.add_argument('--lr', type=float, default=0.01)\n",
    "        parser.add_argument('--epochs', type=int, default=1)\n",
    "        parser.add_argument('--runs', type=int, default=1)\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    print(args)\n",
    "\n",
    "    # Your main code (rest of your logic)...\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    node_list = [110223, 146929, 2940, 104544, 62326, 29759, 96890, 47025, 117732, 163206, 61450, 20589, 145422, 33882, 4523, 81254, 82143, 85138, 167093, 125903, 116417, 158097, 95232, 81835, 84070, 53849, 112263, 17237, 34055, 10707, 164818, 32412, 75064, 139893, 161232, 100687, 148029, 55852, 23076, 152166, 44377, 111682, 145090, 132166, 83408, 153864, 143857, 111282, 150656, 36239, 142479, 112753, 149468, 50327, 163800, 45303, 155755, 151189, 160714, 8875, 116826, 142383, 143573, 136795, 277, 120988, 120990, 151641, 44082, 94916, 32814, 117613, 98216, 41936, 16625, 163884, 69260, 29036, 91369, 31451, 161505, 27948, 29235, 15854, 162540, 161899, 157799, 90013, 120275, 57309, 92874, 140989, 11692, 78367, 62285, 80502, 72672, 121790, 12662, 90677]\n",
    "    \n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    data = data.to(device)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "\n",
    "    # Filter the split indices based on your `node_list`\n",
    "    train_idx = torch.tensor([n for n in split_idx['train'] if n in node_list], device=device)\n",
    "    valid_idx = torch.tensor([n for n in split_idx['valid'] if n in node_list], device=device)\n",
    "    test_idx = torch.tensor([n for n in split_idx['test'] if n in node_list], device=device)\n",
    "\n",
    "    print(train_idx)\n",
    "    print(valid_idx)\n",
    "    print(test_idx)\n",
    "\n",
    "    if args.use_sage:\n",
    "        model = SAGE(data.num_features, args.hidden_channels,\n",
    "                     dataset.num_classes, args.num_layers,\n",
    "                     args.dropout).to(device)\n",
    "    else:\n",
    "        model = GCN(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, {'train': train_idx, 'valid': valid_idx, 'test': test_idx}, evaluator, node_list)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3c6a7-1435-4a91-ad82-5c89d5b6cd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
