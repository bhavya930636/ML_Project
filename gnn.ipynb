{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e647c609-3872-4cf9-8453-c031298b9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "from logger import Logger\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, node_list):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    # Create a mask for the specific nodes\n",
    "    mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    mask[node_list] = True  # Set the specific nodes to True\n",
    "\n",
    "    # Ensure split indices are tensors\n",
    "    train_idx = torch.tensor(split_idx['train'], dtype=torch.long)\n",
    "    valid_idx = torch.tensor(split_idx['valid'], dtype=torch.long)\n",
    "    test_idx = torch.tensor(split_idx['test'], dtype=torch.long)\n",
    "\n",
    "    # Use the mask to filter indices correctly for each split\n",
    "    train_mask = mask[train_idx]\n",
    "    valid_mask = mask[valid_idx]\n",
    "    test_mask = mask[test_idx]\n",
    "\n",
    "    # Filter the true and predicted labels\n",
    "    y_true_train = data.y[train_idx][train_mask]\n",
    "    y_pred_train = y_pred[train_idx][train_mask]\n",
    "\n",
    "    y_true_valid = data.y[valid_idx][valid_mask]\n",
    "    y_pred_valid = y_pred[valid_idx][valid_mask]\n",
    "\n",
    "    y_true_test = data.y[test_idx][test_mask]\n",
    "    y_pred_test = y_pred[test_idx][test_mask]\n",
    "\n",
    "    # Debugging outputs\n",
    "    print(f\"Train: {y_true_train.numel()}, {y_pred_train.numel()}\")\n",
    "    print(f\"Valid: {y_true_valid.numel()}, {y_pred_valid.numel()}\")\n",
    "    print(f\"Test: {y_true_test.numel()}, {y_pred_test.numel()}\")\n",
    "\n",
    "    # Evaluate using the filtered true and predicted labels\n",
    "    train_acc = evaluator.eval({'y_true': y_true_train, 'y_pred': y_pred_train})['acc'] if y_true_train.numel() > 0 else 0.0\n",
    "    valid_acc = evaluator.eval({'y_true': y_true_valid, 'y_pred': y_pred_valid})['acc'] if y_true_valid.numel() > 0 else 0.0\n",
    "    test_acc = evaluator.eval({'y_true': y_true_test, 'y_pred': y_pred_test})['acc'] if y_true_test.numel() > 0 else 0.0\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae993947-5063-4250-86a2-b52c3a03b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.main.<locals>.Args object at 0x7efc0042ecd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kriti-arora/.local/lib/python3.8/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2940,  47025,  96890, 104544, 110223, 163206])\n",
      "tensor([])\n",
      "tensor([ 29759,  62326, 117732, 146929])\n",
      "Train: 6, 6\n",
      "Valid: 0, 0\n",
      "Test: 4, 4\n",
      "Run: 01, Epoch: 01, Loss: 3.9765, Train: 33.33%, Valid: 0.00%, Test: 25.00%\n",
      "Run 01:\n",
      "Highest Train: 33.33\n",
      "Highest Valid: 0.00\n",
      "  Final Train: 33.33\n",
      "   Final Test: 25.00\n",
      "All runs:\n",
      "Highest Train: 33.33 ± nan\n",
      "Highest Valid: 0.00 ± nan\n",
      "  Final Train: 33.33 ± nan\n",
      "   Final Test: 25.00 ± nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3971/2671334735.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_idx = torch.tensor(split_idx['train'], dtype=torch.long)\n",
      "/tmp/ipykernel_3971/2671334735.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_idx = torch.tensor(split_idx['valid'], dtype=torch.long)\n",
      "/tmp/ipykernel_3971/2671334735.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_idx = torch.tensor(split_idx['test'], dtype=torch.long)\n",
      "/home/kriti-arora/kriti/college/sem5/ML/project/ML_Project/logger.py:38: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
      "/home/kriti-arora/kriti/college/sem5/ML/project/ML_Project/logger.py:40: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
      "/home/kriti-arora/kriti/college/sem5/ML/project/ML_Project/logger.py:42: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
      "/home/kriti-arora/kriti/college/sem5/ML/project/ML_Project/logger.py:44: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Detect if running in Jupyter and simulate argument parsing\n",
    "    if \"ipykernel\" in sys.argv[0]:\n",
    "        class Args:\n",
    "            device = 0\n",
    "            log_steps = 1\n",
    "            use_sage = False\n",
    "            num_layers = 3\n",
    "            hidden_channels = 256\n",
    "            dropout = 0.5\n",
    "            lr = 0.01\n",
    "            epochs = 1\n",
    "            runs = 1\n",
    "        args = Args()\n",
    "    else:\n",
    "        # Argument parsing for command-line execution\n",
    "        parser = argparse.ArgumentParser(description='OGBN-Arxiv (GNN)')\n",
    "        parser.add_argument('--device', type=int, default=0)\n",
    "        parser.add_argument('--log_steps', type=int, default=1)\n",
    "        parser.add_argument('--use_sage', action='store_true')\n",
    "        parser.add_argument('--num_layers', type=int, default=3)\n",
    "        parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "        parser.add_argument('--dropout', type=float, default=0.5)\n",
    "        parser.add_argument('--lr', type=float, default=0.01)\n",
    "        parser.add_argument('--epochs', type=int, default=500)\n",
    "        parser.add_argument('--runs', type=int, default=10)\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    print(args)\n",
    "\n",
    "    # Your main code (rest of your logic)...\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    node_list = [110223, 146929, 2940, 104544, 62326, 29759, 96890, 47025, 117732, 163206]\n",
    "    \n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    data = data.to(device)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "\n",
    "    # Filter the split indices based on your `node_list`\n",
    "    train_idx = torch.tensor([n for n in split_idx['train'] if n in node_list], device=device)\n",
    "    valid_idx = torch.tensor([n for n in split_idx['valid'] if n in node_list], device=device)\n",
    "    test_idx = torch.tensor([n for n in split_idx['test'] if n in node_list], device=device)\n",
    "\n",
    "    print(train_idx)\n",
    "    print(valid_idx)\n",
    "    print(test_idx)\n",
    "\n",
    "    if args.use_sage:\n",
    "        model = SAGE(data.num_features, args.hidden_channels,\n",
    "                     dataset.num_classes, args.num_layers,\n",
    "                     args.dropout).to(device)\n",
    "    else:\n",
    "        model = GCN(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, {'train': train_idx, 'valid': valid_idx, 'test': test_idx}, evaluator, node_list)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3c6a7-1435-4a91-ad82-5c89d5b6cd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
