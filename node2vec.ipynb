{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f42dda-91dc-48b5-973f-bc90ef963dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "\n",
    "def save_embedding(model):\n",
    "    torch.save(model.embedding.weight.data.cpu(), 'embedding.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a21758-da1e-41d2-b203-df579f75c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.main.<locals>.Args object at 0x7f5c0fe91280>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m         save_embedding(model)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m specific_nodes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(specific_nodes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Check if specific nodes are in the original dataset\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m existing_nodes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print specific nodes and check if they exist\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecific nodes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, specific_nodes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if \"ipykernel\" in sys.argv[0]:\n",
    "        class Args:\n",
    "            def __init__(self):\n",
    "                self.embedding_dim = 128  # Set your desired embedding dimension\n",
    "                self.walk_length = 10      # Set your desired walk length\n",
    "                self.context_size = 5      # Set your desired context size\n",
    "                self.walks_per_node = 5    # Set your desired number of walks per node\n",
    "                self.batch_size = 64        # Set your desired batch size\n",
    "                self.lr = 0.01              # Set your desired learning rate\n",
    "                self.epochs = 100           # Set your desired number of epochs\n",
    "                self.log_steps = 10         # Set your desired logging frequency\n",
    "        \n",
    "        # Create an instance of Args\n",
    "        args = Args()\n",
    "\n",
    "    else:\n",
    "        # Argument parsing for command-line execution\n",
    "        parser = argparse.ArgumentParser(description='OGBN-Arxiv (GNN)')\n",
    "        parser.add_argument('--device', type=int, default=0)\n",
    "        parser.add_argument('--log_steps', type=int, default=1)\n",
    "        parser.add_argument('--use_sage', action='store_true')\n",
    "        parser.add_argument('--num_layers', type=int, default=3)\n",
    "        parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "        parser.add_argument('--dropout', type=float, default=0.5)\n",
    "        parser.add_argument('--lr', type=float, default=0.01)\n",
    "        parser.add_argument('--epochs', type=int, default=500)\n",
    "        parser.add_argument('--runs', type=int, default=10)\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    print(args)\n",
    "    \n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "    data = dataset[0]\n",
    "    data.edge_index = to_undirected(data.edge_index, data.num_nodes)\n",
    "\n",
    "\n",
    "    # Create a subgraph consisting only of the specific nodes\n",
    "    specific_nodes = [110223, 146929, 2940, 104544, 62326, 29759, 96890, 47025]  # Your actual specific nodes\n",
    "    specific_nodes_tensor = torch.tensor(specific_nodes, dtype=torch.long)\n",
    "    \n",
    "    # Check if specific nodes are in the original dataset\n",
    "    existing_nodes = torch.unique(data.edge_index)\n",
    "    \n",
    "    # Print specific nodes and check if they exist\n",
    "    print(\"Specific nodes:\", specific_nodes)\n",
    "    print(\"Existing nodes in dataset:\", existing_nodes.tolist())\n",
    "    \n",
    "    # Create edge index mask\n",
    "    mask = torch.isin(edge_index[0], specific_nodes_tensor) & torch.isin(edge_index[1], specific_nodes_tensor)\n",
    "    \n",
    "    # Check if mask is empty\n",
    "    print(\"Mask shape:\", mask.shape)\n",
    "    print(\"Number of edges in subgraph:\", mask.sum().item())\n",
    "    \n",
    "    sub_edge_index = edge_index[:, mask]\n",
    "    \n",
    "    # Ensure the sub_edge_index is not empty before proceeding\n",
    "    if sub_edge_index.numel() == 0:\n",
    "        print(\"No edges found for the specified nodes.\")\n",
    "    else:\n",
    "        # Create the subgraph and adjust node features and labels\n",
    "        sub_data = data.clone()\n",
    "        sub_data.edge_index = sub_edge_index\n",
    "        sub_data.x = data.x[specific_nodes_tensor]  # Update the node features\n",
    "        sub_data.y = data.y[specific_nodes_tensor] if data.y is not None else None  # Update the labels if they exist\n",
    "    \n",
    "        # Ensure the edge index is undirected\n",
    "        sub_data.edge_index = to_undirected(sub_data.edge_index)\n",
    "    \n",
    "        # Proceed with Node2Vec initialization\n",
    "        model = Node2Vec(sub_data.edge_index, args.embedding_dim, args.walk_length,\n",
    "                         args.context_size, args.walks_per_node,\n",
    "                         sparse=True).to(device)\n",
    "    \n",
    "        # Create a loader for the subgraph\n",
    "        loader = model.loader(batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=args.lr)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % args.log_steps == 0:\n",
    "                print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, Loss: {loss:.4f}')\n",
    "\n",
    "            if (i + 1) % 100 == 0:  # Save model every 100 steps.\n",
    "                save_embedding(model)\n",
    "        save_embedding(model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1163b35-df73-4d33-911a-9aca5078de38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
