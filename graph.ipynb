{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127a1099-17be-41da-8047-eec08e23c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from utils import loadWord2Vec, clean_str\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a607a6-4231-4009-af7f-84af59a2cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_numbers = [110224, 146930, 2941, 104545, 62327, 29760, 96891, 47026, 117733, 163207, 61451, 20590, 145423, 33883, 4524, 81255, 82144, 85139, 167094, 125904, 116418, 158098, 95233, 81836, 84071, 53850, 112264, 17238, 34056, 10708, 164819, 32413, 75065, 139894, 161233, 100688, 148030, 55853, 23077, 152167, 44378, 111683, 145091, 132167, 83409, 153865, 143858, 111283, 150657, 36240, 142480, 112754, 149469, 50328, 163801, 45304, 155756, 151190, 160715, 8876, 116827, 142384, 143574, 136796, 278, 120989, 120991, 151642, 44083, 94917, 32815, 117614, 98217, 41937, 16626, 163885, 69261, 29037, 91370, 31452, 161506, 27949, 29236, 15855, 162541, 161900, 157800, 90014, 120276, 57310, 92875, 140990, 11693, 78368, 62286, 80503, 72673, 121791, 12663, 90678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5406d7f-94b7-4868-b5fa-1bf28a2a2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids=[]\n",
    "for line_no in line_numbers:\n",
    "    node_id = line_no - 1\n",
    "    node_ids.append(node_id)\n",
    "\n",
    "node_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4505bc-633b-4e26-86a6-3eed576b7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277, 2940, 4523, 8875, 10707]\n"
     ]
    }
   ],
   "source": [
    "print(node_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0929f6-5d08-4f46-b737-4314440e839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['277\\ttrain\\t16', '2940\\ttrain\\t24', '4523\\ttrain\\t30', '8875\\ttrain\\t24', '10707\\ttrain\\t28', '11692\\ttrain\\t4', '12662\\ttrain\\t24', '15854\\ttrain\\t8', '16625\\ttrain\\t16', '17237\\ttrain\\t16', '20589\\ttrain\\t27', '23076\\ttrain\\t28', '27948\\ttrain\\t10', '29036\\ttrain\\t28', '29235\\ttrain\\t5', '29759\\ttrain\\t4', '31451\\ttrain\\t23', '32412\\ttrain\\t39', '32814\\ttrain\\t2', '33882\\ttrain\\t2', '34055\\ttrain\\t34', '36239\\ttrain\\t25', '41936\\ttrain\\t20', '44082\\ttrain\\t32', '44377\\ttrain\\t16', '45303\\ttrain\\t4', '47025\\ttrain\\t34', '50327\\ttrain\\t24', '53849\\ttrain\\t28', '55852\\ttrain\\t24', '57309\\ttrain\\t31', '61450\\ttrain\\t30', '62285\\ttrain\\t25', '62326\\ttrain\\t28', '69260\\ttrain\\t19', '72672\\ttrain\\t22', '75064\\ttrain\\t16', '78367\\ttrain\\t26', '80502\\ttrain\\t24', '81254\\ttrain\\t27', '81835\\ttrain\\t11', '82143\\ttrain\\t28', '83408\\ttrain\\t28', '84070\\ttrain\\t16', '85138\\ttrain\\t19', '90013\\ttrain\\t34', '90677\\ttrain\\t4', '91369\\ttrain\\t31', '92874\\ttrain\\t4', '94916\\ttrain\\t24', '95232\\ttrain\\t24', '96890\\ttrain\\t5', '98216\\ttrain\\t39', '100687\\ttrain\\t18', '104544\\ttrain\\t28', '110223\\ttrain\\t10', '111282\\ttrain\\t16', '111682\\ttrain\\t39', '112263\\ttrain\\t13', '112753\\ttrain\\t28', '116417\\ttrain\\t16', '116826\\ttrain\\t3', '117613\\ttrain\\t16', '117732\\ttrain\\t24', '120275\\ttrain\\t31', '120988\\ttrain\\t16', '120990\\ttrain\\t28', '121790\\ttrain\\t15', '125903\\ttrain\\t20', '132166\\ttrain\\t27', '136795\\ttrain\\t16', '139893\\ttrain\\t8', '140989\\ttrain\\t24', '142383\\ttrain\\t4', '142479\\ttrain\\t33', '143573\\ttrain\\t31', '143857\\ttrain\\t24', '145090\\ttrain\\t8', '145422\\ttrain\\t24', '146929\\ttrain\\t27', '148029\\ttrain\\t16', '149468\\ttrain\\t30', '150656\\ttrain\\t5', '151189\\ttrain\\t28', '151641\\ttrain\\t16', '152166\\ttrain\\t26', '153864\\ttrain\\t13', '155755\\ttrain\\t28', '157799\\ttrain\\t7', '158097\\ttrain\\t16', '160714\\ttrain\\t39', '161232\\ttrain\\t16', '161505\\ttrain\\t5', '161899\\ttrain\\t28', '162540\\ttrain\\t19', '163206\\ttrain\\t24', '163800\\ttrain\\t24', '163884\\ttrain\\t16', '164818\\ttrain\\t16', '167093\\ttrain\\t28']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/graph_node_labels.txt', 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "# Process only the lines with the given indices\n",
    "for i, line in enumerate(lines):\n",
    "    if i in node_ids:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "f.close()\n",
    "print(doc_train_list)\n",
    "print(doc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a187027-4bc8-4f00-bc80-727d972233d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['construction q constant weight sequences using like approach present scheme constant weight sequences , , given information sequence , construction results sequence specific weight within certain scheme uses design based codes , weight sequences , applications', 'decision study agent population identify supported proposal change , space deliberation process agents , two new proposal , agents identify class spaces , spaces , deliberation process result success deliberation , long supported proposal consider general proposal space deliberation setting study conditions deliberation success analysis dynamic algorithm identify supported proposal', 'decentralized dynamic optimization power network voltage control voltage control power distribution networks devices devices also provide limited power resources used network wide voltage decentralized voltage control strategy voltage mismatch error using gradient \\\\( \\\\) power network , local voltage provide gradient information paper aims analyze performance decentralized based voltage control design two dynamic scenarios \\\\) nodes perform decentralized , \\\\) network time varying voltage control , improve existing convergence voltage based gradient modeling network using process time varying constraints , provide error bound tracking optimal solution error result extended general dynamic optimization problems stochastic processes bounded changes numerical demonstrate results realistic power networks', 'reduction paper , propose procedure given \\\\( \\\\) , language deterministic one b size size bound number , used , obtained', 'rate lower bounds distributed storage one distributed storage system large amount source data long using large number n storage nodes , capacity storage system capacity available , e , 1 n storage nodes time nodes , thus data system average rate n , 1 average node source data , data network nodes average rate , data nodes based data main result , , source data point time must case 2 n provides lower bound average rate data system order source data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/clean_100.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "print(doc_content_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f22516-f429-4c96-86b6-95e1ce3a2891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[]\n",
      "[46, 27, 12, 35, 10, 94, 50, 88, 7, 72, 18, 95, 16, 26, 55, 97, 70, 11, 91, 51, 3, 36, 2, 75, 1, 78, 30, 89, 76, 53, 63, 0, 17, 44, 38, 74, 52, 32, 34, 21, 6, 71, 82, 62, 9, 33, 99, 80, 81, 24, 37, 86, 69, 65, 42, 64, 48, 22, 61, 14, 20, 39, 66, 84, 23, 8, 4, 85, 31, 13, 90, 5, 57, 58, 92, 40, 54, 98, 60, 45, 87, 25, 73, 19, 67, 49, 29, 41, 93, 47, 68, 83, 56, 15, 77, 28, 43, 79, 59, 96]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "doc_content_list = []\n",
    "f = open('data/corpus/clean_100.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('data/train_100.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('data/test_100.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "print(ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb722885-eaa0-4437-b32f-831daab78dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90677\\ttrain\\t4', '50327\\ttrain\\t24', '27948\\ttrain\\t10', '72672\\ttrain\\t22', '20589\\ttrain\\t27', '162540\\ttrain\\t19', '95232\\ttrain\\t24', '157799\\ttrain\\t7', '15854\\ttrain\\t8', '140989\\ttrain\\t24', '32814\\ttrain\\t2', '163206\\ttrain\\t24', '31451\\ttrain\\t23', '47025\\ttrain\\t34', '110223\\ttrain\\t10', '163884\\ttrain\\t16', '136795\\ttrain\\t16', '23076\\ttrain\\t28', '161232\\ttrain\\t16', '96890\\ttrain\\t5', '8875\\ttrain\\t24', '75064\\ttrain\\t16', '4523\\ttrain\\t30', '143573\\ttrain\\t31', '2940\\ttrain\\t24', '145422\\ttrain\\t24', '57309\\ttrain\\t31', '158097\\ttrain\\t16', '143857\\ttrain\\t24', '100687\\ttrain\\t18', '117732\\ttrain\\t24', '277\\ttrain\\t16', '32412\\ttrain\\t39', '85138\\ttrain\\t19', '80502\\ttrain\\t24', '142479\\ttrain\\t33', '98216\\ttrain\\t39', '62285\\ttrain\\t25', '69260\\ttrain\\t19', '36239\\ttrain\\t25', '12662\\ttrain\\t24', '139893\\ttrain\\t8', '150656\\ttrain\\t5', '117613\\ttrain\\t16', '17237\\ttrain\\t16', '62326\\ttrain\\t28', '167093\\ttrain\\t28', '148029\\ttrain\\t16', '149468\\ttrain\\t30', '44377\\ttrain\\t16', '78367\\ttrain\\t26', '153864\\ttrain\\t13', '132166\\ttrain\\t27', '120988\\ttrain\\t16', '83408\\ttrain\\t28', '120275\\ttrain\\t31', '92874\\ttrain\\t4', '41936\\ttrain\\t20', '116826\\ttrain\\t3', '29235\\ttrain\\t5', '34055\\ttrain\\t34', '81254\\ttrain\\t27', '120990\\ttrain\\t28', '151641\\ttrain\\t16', '44082\\ttrain\\t32', '16625\\ttrain\\t16', '10707\\ttrain\\t28', '152166\\ttrain\\t26', '61450\\ttrain\\t30', '29036\\ttrain\\t28', '160714\\ttrain\\t39', '11692\\ttrain\\t4', '111682\\ttrain\\t39', '112263\\ttrain\\t13', '161505\\ttrain\\t5', '81835\\ttrain\\t11', '104544\\ttrain\\t28', '164818\\ttrain\\t16', '116417\\ttrain\\t16', '90013\\ttrain\\t34', '155755\\ttrain\\t28', '45303\\ttrain\\t4', '142383\\ttrain\\t4', '33882\\ttrain\\t2', '121790\\ttrain\\t15', '94916\\ttrain\\t24', '55852\\ttrain\\t24', '82143\\ttrain\\t28', '161899\\ttrain\\t28', '91369\\ttrain\\t31', '125903\\ttrain\\t20', '151189\\ttrain\\t28', '111282\\ttrain\\t16', '29759\\ttrain\\t4', '145090\\ttrain\\t8', '53849\\ttrain\\t28', '84070\\ttrain\\t16', '146929\\ttrain\\t27', '112753\\ttrain\\t28', '163800\\ttrain\\t24']\n"
     ]
    }
   ],
   "source": [
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('data/shuffle_100.txt', 'w')\n",
    "f.write(shuffle_doc_name_str)\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/shuffle_100.txt', 'w')\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()\n",
    "print(shuffle_doc_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a107d7-8434-4685-9e36-bdfbf405df88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studied',\n",
       " 'distortion',\n",
       " 'generalization',\n",
       " 'must',\n",
       " 'query',\n",
       " 'perspective',\n",
       " 'continuous',\n",
       " 'application',\n",
       " 'loss',\n",
       " 'issue',\n",
       " 'synthesized',\n",
       " 'layer',\n",
       " 'explicitly',\n",
       " 'recommendation',\n",
       " 'conventional',\n",
       " 'technique',\n",
       " 'privacy',\n",
       " 'filter',\n",
       " 'comprehension',\n",
       " '0',\n",
       " 'sample',\n",
       " 'short',\n",
       " 'eavesdropper',\n",
       " 'literature',\n",
       " 'ratio',\n",
       " 'specific',\n",
       " 'end',\n",
       " 'communication',\n",
       " 'actually',\n",
       " 'rates',\n",
       " 'like',\n",
       " 'computing',\n",
       " 'top',\n",
       " 'computer',\n",
       " 'weak',\n",
       " 'inference',\n",
       " 'finally',\n",
       " 'binary',\n",
       " 'synthetic',\n",
       " 'users',\n",
       " 'large',\n",
       " 'occlusion',\n",
       " 'compare',\n",
       " 'reduction',\n",
       " 'single',\n",
       " 'test',\n",
       " 'recently',\n",
       " 'weight',\n",
       " 'estimation',\n",
       " 'parameters',\n",
       " 'parts',\n",
       " 'independent',\n",
       " 'modeling',\n",
       " 'problems',\n",
       " 'transmitter',\n",
       " 'mismatch',\n",
       " 'proof',\n",
       " 'proposal',\n",
       " '\\\\)',\n",
       " 'allowing',\n",
       " 'recurrent',\n",
       " 'vectors',\n",
       " 'constraints',\n",
       " 'state',\n",
       " 'electricity',\n",
       " 'benefits',\n",
       " 'hard',\n",
       " 'practice',\n",
       " 'emotion',\n",
       " 'basis',\n",
       " 'speech',\n",
       " 'give',\n",
       " 'complexity',\n",
       " 'cvae',\n",
       " 'extension',\n",
       " 'defined',\n",
       " 'supported',\n",
       " 'coding',\n",
       " 'dynamic',\n",
       " 'well',\n",
       " 'function',\n",
       " 'networks',\n",
       " 'unsupervised',\n",
       " 'structured',\n",
       " 'setting',\n",
       " 'strategy',\n",
       " 'divergence',\n",
       " 'factors',\n",
       " 'annotation',\n",
       " 'human',\n",
       " 'make',\n",
       " 'efficiency',\n",
       " 'train',\n",
       " 'used',\n",
       " 'change',\n",
       " 'signals',\n",
       " 'thus',\n",
       " 'instance',\n",
       " 'special',\n",
       " 'sentence',\n",
       " 'scheme',\n",
       " 'online',\n",
       " 'dataset',\n",
       " 'new',\n",
       " 'real',\n",
       " 'applications',\n",
       " 'datasets',\n",
       " 'us',\n",
       " 'complete',\n",
       " 'also',\n",
       " 'voltage',\n",
       " 'regret',\n",
       " 'assignment',\n",
       " 'effective',\n",
       " 'neural',\n",
       " 'states',\n",
       " 'challenging',\n",
       " 'framework',\n",
       " 'convolutional',\n",
       " 'capture',\n",
       " 'unknown',\n",
       " 'intelligence',\n",
       " 'question',\n",
       " 'component',\n",
       " 'able',\n",
       " 'structure',\n",
       " 'lstm',\n",
       " 'different',\n",
       " 'set',\n",
       " 'paper',\n",
       " 'energy',\n",
       " 'first',\n",
       " 'queries',\n",
       " 'approximation',\n",
       " 'means',\n",
       " 'semantics',\n",
       " 'either',\n",
       " 'conditions',\n",
       " 'n',\n",
       " 'pixels',\n",
       " 'gaussian',\n",
       " 'error',\n",
       " 'across',\n",
       " 'contrastive',\n",
       " 'rate',\n",
       " 'robust',\n",
       " 'bounded',\n",
       " 'space',\n",
       " 'sequences',\n",
       " 'overall',\n",
       " 'describes',\n",
       " 'understanding',\n",
       " 'environment',\n",
       " 'consider',\n",
       " 'sparse',\n",
       " 'based',\n",
       " 'tag',\n",
       " 'generative',\n",
       " 'since',\n",
       " 'renyi',\n",
       " 'issues',\n",
       " 'ropnn',\n",
       " 'changes',\n",
       " 'similarity',\n",
       " 'multiple',\n",
       " 'assumption',\n",
       " 'graph',\n",
       " 'exploit',\n",
       " 'alignment',\n",
       " 'visual',\n",
       " 'categorical',\n",
       " 'available',\n",
       " 'support',\n",
       " 'matrices',\n",
       " 'fixed',\n",
       " 'called',\n",
       " 'use',\n",
       " 'proposed',\n",
       " 'depth',\n",
       " 'explanation',\n",
       " 'computational',\n",
       " 'significant',\n",
       " 'learning',\n",
       " 'address',\n",
       " 'patterns',\n",
       " 'demonstrate',\n",
       " 'improving',\n",
       " 'long',\n",
       " 'regulation',\n",
       " 'method',\n",
       " 'options',\n",
       " 'domain',\n",
       " 'decentralized',\n",
       " 'addition',\n",
       " 'input',\n",
       " 'unified',\n",
       " 'achieves',\n",
       " 'represent',\n",
       " ',',\n",
       " 'systematic',\n",
       " 'per',\n",
       " 'fine',\n",
       " 'within',\n",
       " 'structures',\n",
       " 'disaggregation',\n",
       " 'matrix',\n",
       " 'resources',\n",
       " 'introduced',\n",
       " 'recent',\n",
       " 'device',\n",
       " 'relevance',\n",
       " 'orthogonal',\n",
       " 'distributed',\n",
       " 'mapping',\n",
       " 'sets',\n",
       " 'adaptive',\n",
       " 'multi',\n",
       " 'trained',\n",
       " 'transfer',\n",
       " 'words',\n",
       " 'finite',\n",
       " 'among',\n",
       " 'step',\n",
       " 'additional',\n",
       " 'adversarial',\n",
       " 'theory',\n",
       " 'time',\n",
       " 'evaluate',\n",
       " 'part',\n",
       " 'model',\n",
       " 'ability',\n",
       " 'objects',\n",
       " 'main',\n",
       " 'deliberation',\n",
       " 'representations',\n",
       " 'terms',\n",
       " 'video',\n",
       " 'challenge',\n",
       " 'rnns',\n",
       " 'simple',\n",
       " 'critic',\n",
       " 'inducing',\n",
       " 'object',\n",
       " 'traffic',\n",
       " 'expressions',\n",
       " 'system',\n",
       " 'view',\n",
       " 'x',\n",
       " 'security',\n",
       " 'stable',\n",
       " 'uses',\n",
       " 'perception',\n",
       " 'algorithms',\n",
       " 'unique',\n",
       " 'current',\n",
       " 'rather',\n",
       " 'identify',\n",
       " 'important',\n",
       " 'relay',\n",
       " 'therefore',\n",
       " 'natural',\n",
       " 'embeddings',\n",
       " 'machine',\n",
       " 'actors',\n",
       " 'order',\n",
       " 'autoencoder',\n",
       " 'practical',\n",
       " 'code',\n",
       " 'directly',\n",
       " 'scalability',\n",
       " 'mask',\n",
       " 'approaches',\n",
       " 'neighbor',\n",
       " 'fsa',\n",
       " 'analyze',\n",
       " 'transmission',\n",
       " 'action',\n",
       " 'design',\n",
       " 'complex',\n",
       " 'including',\n",
       " 'rop',\n",
       " 'one',\n",
       " 'insights',\n",
       " 'techniques',\n",
       " 'sequence',\n",
       " 'social',\n",
       " 'consistency',\n",
       " 'estimate',\n",
       " 'image',\n",
       " 'k',\n",
       " 'tasks',\n",
       " 'bottom',\n",
       " '\\\\(',\n",
       " 'present',\n",
       " 'selection',\n",
       " 'community',\n",
       " 'graphs',\n",
       " 'em',\n",
       " 'games',\n",
       " 'processing',\n",
       " 'manner',\n",
       " 'numerical',\n",
       " 'reading',\n",
       " 'attention',\n",
       " 'features',\n",
       " 'underlying',\n",
       " 'scenarios',\n",
       " 'providing',\n",
       " 'three',\n",
       " 'source',\n",
       " 'varying',\n",
       " 'various',\n",
       " 'given',\n",
       " 'description',\n",
       " 'algorithm',\n",
       " 'level',\n",
       " 'prior',\n",
       " 'properties',\n",
       " 'reconstruction',\n",
       " 'capacity',\n",
       " 'arbitrary',\n",
       " 'constraint',\n",
       " 'software',\n",
       " 'motivated',\n",
       " 'p',\n",
       " 'vision',\n",
       " 'interaction',\n",
       " 'interpretable',\n",
       " 'supervised',\n",
       " \"'s\",\n",
       " 'obtained',\n",
       " 'annotated',\n",
       " 'policy',\n",
       " 'spaces',\n",
       " 'feedback',\n",
       " 'samples',\n",
       " 'accuracy',\n",
       " 'specifically',\n",
       " 'measurement',\n",
       " 'due',\n",
       " 'detection',\n",
       " 'analysis',\n",
       " 'art',\n",
       " 'devices',\n",
       " 'rip',\n",
       " 'using',\n",
       " 'distributions',\n",
       " 'problem',\n",
       " 'developed',\n",
       " 'prediction',\n",
       " 'greedy',\n",
       " 'without',\n",
       " 'phase',\n",
       " 'semantic',\n",
       " 'baseline',\n",
       " 'charging',\n",
       " 'smart',\n",
       " 'two',\n",
       " 'node',\n",
       " 'possible',\n",
       " 'length',\n",
       " 'designing',\n",
       " 'experimental',\n",
       " 'models',\n",
       " 'presented',\n",
       " 'via',\n",
       " 'power',\n",
       " 'faster',\n",
       " 'guidance',\n",
       " 'training',\n",
       " 'deep',\n",
       " 'feature',\n",
       " 'optimal',\n",
       " 'linear',\n",
       " 'metadata',\n",
       " 'q',\n",
       " 'bits',\n",
       " 'way',\n",
       " 'research',\n",
       " 'images',\n",
       " 'storage',\n",
       " 'limited',\n",
       " 'e',\n",
       " 'popular',\n",
       " 'named',\n",
       " 'convergence',\n",
       " 'quality',\n",
       " 'alternative',\n",
       " 'processes',\n",
       " 'random',\n",
       " 'throughput',\n",
       " 'cl',\n",
       " 'key',\n",
       " 'would',\n",
       " 'optimization',\n",
       " 'improves',\n",
       " 'hybrid',\n",
       " 'introduce',\n",
       " 'methods',\n",
       " 'propose',\n",
       " 'uic',\n",
       " 'procedure',\n",
       " 'article',\n",
       " 'achieve',\n",
       " 'derived',\n",
       " 'network',\n",
       " 'standard',\n",
       " 'scratch',\n",
       " 'comprehensive',\n",
       " 'signal',\n",
       " 'memory',\n",
       " 'related',\n",
       " 'codes',\n",
       " '\\\\?',\n",
       " 'streaming',\n",
       " 'latent',\n",
       " 'bounds',\n",
       " 'content',\n",
       " 'interference',\n",
       " 'salient',\n",
       " 'fiducial',\n",
       " 'deterministic',\n",
       " 'architecture',\n",
       " 'logic',\n",
       " 'find',\n",
       " 'rule',\n",
       " 'search',\n",
       " 'provides',\n",
       " 'may',\n",
       " 'allows',\n",
       " 'individual',\n",
       " 'even',\n",
       " 'term',\n",
       " 'capability',\n",
       " 'systems',\n",
       " 'url',\n",
       " 'sorter',\n",
       " 'distribution',\n",
       " 'sensor',\n",
       " 'particular',\n",
       " 'measurements',\n",
       " 'novel',\n",
       " '2',\n",
       " 'knowledge',\n",
       " 'work',\n",
       " 'bound',\n",
       " 'focus',\n",
       " 'hence',\n",
       " 'gradient',\n",
       " 'variational',\n",
       " 'show',\n",
       " 'equilibrium',\n",
       " 'secrecy',\n",
       " 'entropy',\n",
       " 'deformation',\n",
       " 'number',\n",
       " 'spectral',\n",
       " 'grid',\n",
       " 'qoe',\n",
       " 'role',\n",
       " 'mesh',\n",
       " 'testing',\n",
       " 'define',\n",
       " 'however',\n",
       " 'aims',\n",
       " 'task',\n",
       " 'classification',\n",
       " 'b',\n",
       " 'interest',\n",
       " 'control',\n",
       " 'cues',\n",
       " 'realistic',\n",
       " 'perform',\n",
       " 'recognition',\n",
       " 'class',\n",
       " 'general',\n",
       " 'metrics',\n",
       " 'sub',\n",
       " 'many',\n",
       " 'game',\n",
       " 'performance',\n",
       " 'word',\n",
       " 'effectively',\n",
       " 'ergodic',\n",
       " 'describing',\n",
       " 'success',\n",
       " 'tla',\n",
       " 'agents',\n",
       " 'demonstrations',\n",
       " 'maximum',\n",
       " 'self',\n",
       " 'evaluation',\n",
       " 'theoretic',\n",
       " 'previous',\n",
       " 'learn',\n",
       " 'adaptation',\n",
       " 'high',\n",
       " 'experiments',\n",
       " 'channel',\n",
       " 'edge',\n",
       " 'provide',\n",
       " 'quantum',\n",
       " 'saliency',\n",
       " 'variable',\n",
       " 'user',\n",
       " 'privileged',\n",
       " 'block',\n",
       " 'improve',\n",
       " 'approach',\n",
       " 'second',\n",
       " 'text',\n",
       " 'extended',\n",
       " 'constant',\n",
       " 'local',\n",
       " 'data',\n",
       " 'cognitive',\n",
       " 'g',\n",
       " 'every',\n",
       " 'resolution',\n",
       " 'amount',\n",
       " 'noise',\n",
       " 'case',\n",
       " 'small',\n",
       " 'job',\n",
       " 'several',\n",
       " 'nodes',\n",
       " 'attack',\n",
       " 'cost',\n",
       " 'result',\n",
       " 'domains',\n",
       " 'generate',\n",
       " 'leverage',\n",
       " 'mid',\n",
       " 'option',\n",
       " 'significantly',\n",
       " 'likelihood',\n",
       " 'disaggregated',\n",
       " 'trust',\n",
       " 'existing',\n",
       " 'recovery',\n",
       " 'known',\n",
       " 'active',\n",
       " 'gci',\n",
       " 'solution',\n",
       " '1',\n",
       " 'average',\n",
       " 'rnn',\n",
       " 'information',\n",
       " 'context',\n",
       " 'process',\n",
       " 'target',\n",
       " 'certain',\n",
       " 'could',\n",
       " 'occluded',\n",
       " 'cloud',\n",
       " 'nonlinear',\n",
       " 'generic',\n",
       " 'world',\n",
       " 'scale',\n",
       " 'sensing',\n",
       " 'results',\n",
       " 'size',\n",
       " 'shown',\n",
       " 'better',\n",
       " 'agent',\n",
       " 'channels',\n",
       " 'bottleneck',\n",
       " 'supervision',\n",
       " 'studies',\n",
       " 'population',\n",
       " 'probability',\n",
       " 'higher',\n",
       " 'learned',\n",
       " 'temporal',\n",
       " 'compared',\n",
       " 'adapt',\n",
       " 'embedding',\n",
       " 'filtering',\n",
       " 'heuristic',\n",
       " 'seen',\n",
       " 'construction',\n",
       " 'grained',\n",
       " 'become',\n",
       " 'non',\n",
       " 'cases',\n",
       " 'lower',\n",
       " 'mechanism',\n",
       " 'language',\n",
       " 'value',\n",
       " 'frequency',\n",
       " 'synthesis',\n",
       " 'driven',\n",
       " 'lie',\n",
       " 'highly',\n",
       " 'achieved',\n",
       " 'study',\n",
       " 'nearest',\n",
       " 'r',\n",
       " 'decision',\n",
       " 'representation',\n",
       " 'wide',\n",
       " 'good',\n",
       " 'considered',\n",
       " 'less',\n",
       " 'point',\n",
       " 'universal',\n",
       " 'tracking',\n",
       " 'similar',\n",
       " 'efficient',\n",
       " 'effectiveness',\n",
       " 'scheduling',\n",
       " 'topic',\n",
       " 'stochastic']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb13dc6-26d9-4c85-b345-7660074fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "f = open('data/corpus/vocab_100.txt', 'w')\n",
    "f.write(vocab_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cb7fb6-0cac-4415-a0ec-9bb585c1a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "f = open('data/corpus/labels_100.txt', 'w')\n",
    "f.write(label_list_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8adda60-fea3-4d0f-a8b9-8103d745a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "# different training rates\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "f = open('data/real_train_100.name', 'w')\n",
    "f.write(real_train_doc_names_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a762e0e8-4e33-46ea-8b80-236d8c820b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "print(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae35e3cb-a117-45be-bc6b-1a068fd9aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 300) (90, 27) (0, 300) (0,) (713, 300) (713, 27)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b9cabe-e372-414d-8d40-72c0c5e46c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4b9f9c-3cef-465f-bb13-e3b1037a431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# word vector cosine similarity as weights\n",
    "\n",
    "'''\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
    "            vector_i = np.array(word_vector_map[vocab[i]])\n",
    "            vector_j = np.array(word_vector_map[vocab[j]])\n",
    "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
    "            if similarity > 0.9:\n",
    "                print(vocab[i], vocab[j], similarity)\n",
    "                row.append(train_size + i)\n",
    "                col.append(train_size + j)\n",
    "                weight.append(similarity)\n",
    "'''\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f74fc5-511a-4600-8cf8-45836dafce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dump objects\n",
    "f = open(\"data/ind.x\", 'wb')\n",
    "pkl.dump(x, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.y\", 'wb')\n",
    "pkl.dump(y, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.tx\", 'wb')\n",
    "pkl.dump(tx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.ty\", 'wb')\n",
    "pkl.dump(ty, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.allx\", 'wb')\n",
    "pkl.dump(allx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.ally\", 'wb')\n",
    "pkl.dump(ally, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.adj\", 'wb')\n",
    "pkl.dump(adj, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d34d9-86da-47c9-b9fa-71d550301beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
