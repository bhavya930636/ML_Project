{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127a1099-17be-41da-8047-eec08e23c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from utils import loadWord2Vec, clean_str\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a607a6-4231-4009-af7f-84af59a2cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_numbers = [30678, 110533, 142793, 72556, 27639, 94748, 18254, 169291, 47676, 160940, 147744, 15127, 78749, 59829, 167317, 73056, 131571, 24531, 118370, 148336, 76346, 165969, 30699, 114240, 32867, 120188, 44423, 106717, 19155, 132223, 12769, 23208, 70517, 75990, 65828, 101146, 27107, 70210, 4574, 89551, 27173, 49976, 149660, 1760, 69609, 49318, 124182, 94595, 123304, 105721, 86350, 159541, 79606, 123028, 107095, 150414, 4310, 64724, 7650, 126763, 135611, 65948, 114278, 94247, 142386, 168140, 128973, 121514, 130405, 34315, 143244, 3719, 61959, 112651, 38171, 148259, 113456, 112232, 73525, 7920, 36853, 116489, 84397, 136277, 123428, 76576, 5151, 56941, 124699, 12533, 18178, 19649, 100381, 78313, 70090, 155329, 135682, 53481, 151208, 130240, 27646, 28675, 79973, 157667, 43100, 53963, 78501, 166603, 151739, 110517, 35089, 132575, 138224, 59489, 90720, 61597, 78225, 10642, 107487, 140943, 126687, 104644, 942, 132278, 85628, 160146, 161788, 39811, 37744, 48316, 985, 152908, 122448, 69097, 16642, 96790, 122168, 524, 74776, 58863, 54978, 62994, 50323, 162333, 96881, 57265, 16134, 20789, 62002, 105838, 152153, 145323, 93092, 116132, 115863, 147098, 123496, 116418, 55670, 120817, 157032, 80543, 158796, 101307, 154457, 151534, 54017, 82708, 117886, 155004, 77893, 15982, 6523, 58206, 103852, 29108, 58625, 94397, 18728, 99859, 62715, 52965, 58689, 121963, 94897, 53898, 81512, 136066, 49889, 149082, 40152, 58837, 131041, 50988, 74057, 27764, 44475, 38533, 138289, 84064, 102196, 109978, 140064, 47479, 90109, 127680, 93724, 49337, 147557, 53527, 128373, 117244, 70030, 83089, 52550, 140977, 114142, 71213, 127228, 25843, 152047, 13642, 39628, 105903, 89399, 70084, 137118, 158140, 53058, 79644, 66067, 107486, 103481, 129807, 87579, 31972, 160747, 92502, 35578, 53687, 111713, 115930, 49946, 102708, 92258, 104793, 18851, 75317, 51949, 78772, 94867, 47699, 27750, 129256, 142778, 165890, 80245, 138370, 162653, 80163, 97246, 84930, 51240, 63005, 73181, 162873, 31275, 52482, 8181, 133766, 16563, 40273, 22473, 120420, 66459, 22057, 110421, 4386, 129445, 7731, 111820, 118364, 19335, 110146, 117208, 84964, 115633, 17821, 57602, 104389, 38097, 32708, 1940, 21765, 66463, 11322, 114730, 141416, 37185, 111174, 151975, 73360, 10505, 161806, 109711, 150952, 126579, 143405, 31718, 159063, 40738, 3444, 89731, 156970, 38965, 120699, 126218, 113026, 48015, 126121, 120351, 86867, 45357, 113806, 56239, 57148, 119563, 160956, 110548, 14329, 141398, 81217, 138079, 159935, 72031, 119773, 34934, 130674, 5133, 51161, 135554, 8974, 156692, 105032, 48168, 117920, 150380, 31060, 121244, 17846, 104319, 106571, 139625, 17689, 59525, 76775, 30207, 13589, 112158, 96661, 128351, 11309, 92158, 12069, 122006, 41350, 117058, 70012, 150684, 19909, 93258, 141485, 137843, 134985, 153276, 84784, 84805, 95727, 24052, 101626, 22299, 8560, 113108, 65544, 71881, 11888, 71135, 24534, 61945, 39243, 90760, 88081, 109415, 2266, 137984, 108391, 100628, 106410, 146810, 31981, 54712, 144855, 162410, 59210, 49629, 21667, 37301, 108070, 63250, 26566, 30367, 64786, 82006, 19291, 23296, 35682, 159070, 20992, 73269, 138294, 119400, 148263, 166258, 68904, 76733, 56974, 143727, 39279, 109316, 164966, 146212, 48376, 132583, 31246, 80483, 166422, 17722, 130626, 110627, 135333, 23544, 156794, 83462, 114633, 59113, 32585, 86595, 109005, 101416, 41735, 43606, 549, 118645, 127733, 119015, 162556, 106641, 88339, 41887, 23722, 93985, 52222, 20946, 69845, 99136, 13277, 42912, 6571, 149813, 166004, 90832, 72292, 100274, 77171, 108849, 31617, 148612, 80328, 66351, 150962, 57683, 29517, 116579, 3499, 54173, 46024, 165345, 98562, 47297, 147219, 44065, 94530, 116856, 35226, 154686, 93235, 21159, 24034, 138048, 75731, 133355, 49697, 58760, 149248, 94908, 25532, 27460, 103466, 105966, 137628, 38619, 60623, 124177, 47753, 41054, 28387, 49755, 38844, 45018, 104682, 43354, 8635, 103029, 75635, 63071, 99741, 69276, 96445, 159806, 168120, 69646, 27017, 5433, 61270, 21658, 67859, 101960, 6852, 8043, 4213, 132016, 166854, 12421, 62468, 116552, 103631, 58902, 31871, 93463, 110023, 53033, 116543, 5008, 53472, 138326, 19934, 14665, 104514, 7684, 43672, 93638, 36960, 22043, 155881, 17349, 114984, 52214, 7774, 56961, 79587, 82866, 136818, 118221, 83351, 90404, 6224, 132913, 94511, 58820, 5838, 128425, 82463, 76730, 50824, 159292, 72199, 91175, 77086, 169227, 140834, 20843, 109400, 73219, 85387, 95050, 80883, 54404, 1868, 68150, 128840, 85706, 82428, 140107, 149630, 6871, 46628, 147416, 126403, 125589, 111473, 118468, 138159, 84762, 26394, 116209, 56213, 154427, 113006, 52360, 54202, 8813, 133122, 133835, 90015, 11491, 34938, 129731, 102046, 161229, 34353, 156672, 131278, 86, 144056, 114050, 5980, 39648, 16151, 54779, 3508, 47576, 91046, 116140, 150372, 47284, 33964, 139780, 97493, 132476, 72709, 80390, 71801, 116465, 79318, 144182, 37531, 150148, 32311, 2829, 95747, 101713, 37683, 146339, 159589, 51438, 95230, 163657, 147892, 114187, 144894, 125855, 50746, 33419, 145894, 57745, 2328, 87991, 28374, 89657, 124441, 104479, 11393, 116221, 35517, 69993, 158190, 57610, 57835, 62503, 61953, 22585, 20808, 59276, 68177, 164469, 88875, 18339, 70305, 167877, 47169, 59916, 142723, 146549, 41083, 35486, 26920, 49192, 132281, 4846, 35383, 155497, 99786, 74699, 32146, 274, 21464, 107257, 89835, 16756, 30799, 93143, 113587, 1127, 118139, 17400, 150591, 44786, 20952, 60259, 163534, 131589, 87967, 124208, 14167, 78446, 138881, 26303, 27760, 44641, 116403, 87921, 57832, 95430, 29952, 16471, 49184, 132119, 108557, 71537, 77242, 76300, 148957, 15392, 81570, 113734, 43444, 131652, 4254, 20864, 57875, 116360, 107956, 52106, 165947, 48615, 80836, 120094, 73012, 50801, 100177, 48212, 61614, 140426, 93418, 159883, 96255, 129280, 2859, 4099, 37981, 80550, 7375, 55920, 43047, 86754, 163668, 99719, 106351, 28487, 63603, 99533, 21327, 32217, 7289, 112590, 391, 135115, 103594, 120258, 133074, 97267, 137679, 56368, 84151, 158710, 159204, 49119, 123764, 97415, 110159, 106172, 167753, 17603, 77021, 8576, 92344, 165212, 45424, 162214, 7588, 113811, 41679, 4401, 51082, 41297, 11770, 148768, 83690, 146905, 127106, 109961, 88908, 25528, 13178, 160222, 92957, 107243, 59179, 75080, 62482, 118435, 132644, 38478, 131584, 108343, 110553, 105993, 71736, 71513, 36810, 138758, 145773, 42537, 5075, 160041, 26738, 61440, 71669, 130603, 144751, 153795, 143055, 73606, 153082, 163789, 91998, 53939, 96922, 135341, 25125, 98910, 150401, 25988, 47515, 34369, 50380, 164094, 275, 69598, 117499, 5200, 39204, 84869, 142496, 152556, 115466, 75752, 66017, 79944, 61009, 80717, 38632, 115446, 37270, 72359, 50060, 146167, 112221, 118904, 128534, 163351, 131247, 147846, 104201, 7972, 89876, 117021, 48315, 109860, 74066, 36909, 110979, 140448, 21280, 59235, 144918, 18120, 124847, 162247, 45961, 100348, 59175, 14079, 139323, 40348, 130168, 32283, 29845, 74328, 144972, 157668, 122277, 166296, 58939, 168814, 102675, 133531, 30870, 109035, 168176, 29466, 66717, 83591, 54120, 162344, 79788, 27248, 113594, 117928, 161569, 71658, 46344, 36540, 7039, 130070, 144759, 156134, 101407, 37281, 129992, 91562, 29195, 101246, 127219, 88106, 153295, 154336, 162031, 150051, 65470, 67594, 22581, 163820, 142078, 110773, 87409, 146180, 58424, 61049, 141686, 155201, 72978, 112239, 110315, 93490, 18665, 164364, 127365, 78934, 120119, 7715, 73584, 135091, 53201, 4527, 107590, 80070, 39682, 118864, 40687, 3739, 73091, 94440, 151414, 127604, 40428, 155272, 142023, 15729, 42530, 147584, 39042, 143064, 136202, 1501, 95524, 115981, 64137, 116411, 84590, 129595, 9652, 25303, 85960, 60079, 162750, 145131, 2272, 4939, 16397, 29686, 74314, 158876, 8756, 58592, 113072, 20771, 28404, 168099, 92508, 123958, 116198, 61573, 143495, 146927, 47557, 158973, 17257, 29789, 25778, 55796, 18377, 154387, 154840, 165105, 118559, 14673, 38040, 99328, 97850, 139335, 3643, 67078, 102739, 23635, 100897, 139110, 66346, 117012, 87820, 978, 10122, 29210, 157615, 123481, 91221, 91902, 58106, 1106, 107031, 4514, 115329, 130905, 26671, 9366, 71799, 103728, 63971, 98477, 122799, 59010, 103570, 158889, 55883, 148383, 18361, 128313, 26697, 160526, 113354, 80400, 90060, 107030, 93580, 33763, 74561, 124777, 5723, 113820, 142670, 13614, 58387, 20621, 56513, 30436, 129516, 86761, 165812, 24597, 102961, 116756, 11922, 45736, 128536, 76307, 26300, 107246, 41583, 141317, 109212, 162922, 23200, 115211, 166664, 59394, 23305, 143114, 141613, 139322, 167474, 118081, 2206, 78879, 23666, 32632, 96356, 820, 115396, 16576, 19933, 26025, 149518, 38396, 73785, 207, 141776, 13565, 140737, 145619, 70401, 153189, 146364, 147848, 165407, 32506, 120666, 19831, 33491, 100868, 103388, 23147, 40287, 153408, 82258, 94023, 39431, 138497, 161444, 112672, 65590, 162260, 58637, 89100, 119063, 108610, 119526, 19850, 58515, 32962, 81943, 46205, 121017, 84479, 15328, 151502, 93077, 89561, 128028, 58821, 149397, 100156, 51112, 107742, 82415, 39493, 34491, 167476, 47519, 130695, 124485, 121262, 84672, 38610, 2228, 139324, 22695, 81, 116725, 90580, 120422, 117598, 166612, 16329, 100063, 63302, 29533, 32950, 61579, 27156, 164452, 157142, 50625, 169038, 99722, 118907, 69759, 168013, 62476, 145957, 22376, 12154, 106367, 101657, 101645, 162446, 40923, 131467, 135721, 30724, 59193, 132623, 161042, 103922, 138068, 143872, 87032, 165032, 77903, 84639, 163652, 143476, 111617, 1321, 51153, 18235, 151203, 133022, 109828, 79336, 141367, 46290, 154869, 26024, 62411, 117836, 156609, 2964, 156647, 94396, 136380, 103382, 162384, 54540, 93832, 44397, 144892, 96611, 32442, 132497, 71167, 15959, 101754, 105350, 107204, 110675, 132316, 80310, 69148, 42616, 87460, 46045, 107292, 149685, 138471, 18776, 95959, 99774, 25176, 62191, 15443, 152511, 165683, 145215, 69378, 696, 26034, 58597, 25078, 95220, 162935, 76359, 139112, 76433, 39892, 81639, 58673, 113043, 69093, 108906, 146710, 157608, 121944, 47067, 17530, 35034, 96853, 18856, 11635, 129401, 134013, 91310, 62240, 77845, 75077, 3240, 38607, 131751, 90603, 168742, 108335, 107365, 52867, 1457, 168879, 141538, 15942, 101018, 164890, 96328, 164339, 26098, 101174, 6453, 29387, 144083, 159026, 17914, 167742, 42087, 165845, 62642, 39976, 11273, 111683, 89967, 90536, 22041, 162349, 163436, 120778, 127815, 27735, 112067, 75903, 148479, 60671, 122034, 106582, 35562, 31306, 163315, 131934, 80037, 150631, 63985, 102464, 8806, 164730, 146766, 48166, 44797, 5139, 18097, 81990, 141095, 25880, 161786, 128797, 133206, 98096, 106265, 46954, 51273, 167630, 57482, 40568, 62013, 128127, 33330, 103283, 135370, 122568, 149224, 150228, 115754, 81557, 157238, 5263, 65208, 13145, 147803, 114059, 20982, 39597, 166277, 119018, 79321, 95570, 60130, 164942, 124523, 34307, 81500, 56953, 167399, 4003, 49285, 133161, 47840, 50425, 142233, 158734, 154722, 46914, 22697, 164004, 148862, 26934, 30188, 95846, 11356, 33299, 149527, 160840, 80202, 54709, 155200, 130359, 109072, 145252, 136177, 15670, 61812, 73719, 41399, 130319, 50997, 124562, 100114, 7635, 25583, 146760, 98577, 106545, 7890, 1069, 57630, 108299, 36027, 81598, 73092, 27620, 96660, 28796, 148091, 107500, 163781, 70458, 46963, 114151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5406d7f-94b7-4868-b5fa-1bf28a2a2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids=[]\n",
    "for line_no in line_numbers:\n",
    "    node_id = line_no - 1\n",
    "    node_ids.append(node_id)\n",
    "\n",
    "node_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4505bc-633b-4e26-86a6-3eed576b7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 85, 206, 273, 274]\n"
     ]
    }
   ],
   "source": [
    "print(node_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0929f6-5d08-4f46-b737-4314440e839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['80\\ttrain\\t28', '57601\\ttrain\\t16', '113455\\tval\\t2']\n",
      "['80\\ttrain\\t28', '80069\\ttrain\\t24', '162383\\ttrain\\t27']\n",
      "['206\\ttest\\t26']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_embeddings_dim = 300\n",
    "word_vector_map = {}\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/all_labels.txt', 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "# Process only the lines with the given indices\n",
    "for i, line in enumerate(lines):\n",
    "    if i in node_ids:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "f.close()\n",
    "print(doc_name_list[::500])\n",
    "print(doc_train_list[::500])\n",
    "print(doc_test_list[::500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a187027-4bc8-4f00-bc80-727d972233d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['late weak markov automata weak bisimilarity distribution based equivalence notion markov automata gained popularity reasonable behavioural equivalence markov automata paper studies strictly notion late weak bisimilarity enjoys valuable properties important subclasses trace distribution equivalence partial information , compositionality preserved distributed intersection two scheduler classes thus still reasonable compositional theory markov automata', 'low delay multi party solution paper , attempt revisit problem multi party practical perspective , design space involved problem believe emphasis low end end delays two parties must , source rate session adapt bandwidth availability congestion present , multi party solution specifically designed achieve objectives entirely peer peer \\\\( \\\\) , eliminating cost maintaining servers designed deliver video low end end delays , quality levels available network resources arbitrary network topologies network contrast commonly assumed scenarios bandwidth edge network highlight design distributed adaptive rate control protocol , discover adapt arbitrary topologies network conditions quickly , converging efficient link rate allocations allowed underlying network adaptive link rate control , source video encoding rates also dynamically controlled optimize video quality arbitrary network conditions implemented prototype system , demonstrate superior performance existing solutions local experimental internet', 'deep learning augmentation expression profiles lack well structured metadata annotations interpretation growing amount publicly available expression data machine learning based prediction metadata \\\\( data augmentation \\\\) considerably improve quality expression data annotation study , systematically benchmark deep learning \\\\( dl \\\\) random forest \\\\( rf \\\\) based metadata augmentation tissue , age , using small \\\\( \\\\) expression profiles use annotated samples small expression \\\\( sea \\\\) database train test augmentation performance general , dl machine learner outperforms rf method almost tested cases average cross validated prediction accuracy dl algorithm 96 5 , 77 , age 77 2 average tissue prediction accuracy completely new dataset 1 \\\\( dl \\\\) 80 8 \\\\( rf \\\\) understand influence dl predictions , employ backpropagation based feature importance scores using method , enable us obtain information biological relevance']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/clean_1500.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "print(doc_content_list[::500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f22516-f429-4c96-86b6-95e1ce3a2891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 4, 5, 6, 9, 10, 12, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 40, 41, 42, 43, 44, 47, 50, 51, 52, 54, 55, 58, 59, 60, 62, 63, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 84, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 113, 114, 115, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 159, 161, 162, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 216, 217, 218, 219, 220, 221, 222, 224, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 239, 240, 245, 246, 247, 249, 250, 253, 254, 255, 256, 258, 259, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 274, 276, 277, 281, 282, 283, 284, 285, 286, 289, 290, 292, 293, 294, 295, 297, 299, 300, 302, 305, 306, 309, 311, 312, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 354, 356, 357, 358, 361, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 389, 390, 392, 397, 398, 399, 402, 403, 404, 405, 406, 409, 410, 411, 413, 414, 415, 417, 418, 419, 420, 421, 422, 423, 425, 426, 427, 428, 429, 430, 431, 434, 435, 436, 437, 438, 439, 441, 443, 445, 446, 447, 448, 449, 451, 452, 454, 455, 456, 457, 458, 459, 462, 463, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 479, 480, 481, 482, 483, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 506, 508, 509, 510, 512, 513, 515, 517, 518, 520, 523, 526, 528, 531, 532, 535, 536, 537, 538, 539, 540, 542, 543, 544, 545, 546, 548, 549, 550, 551, 552, 554, 555, 556, 557, 558, 559, 560, 562, 563, 564, 565, 566, 567, 568, 571, 572, 573, 574, 575, 577, 579, 580, 581, 582, 583, 584, 587, 589, 590, 591, 592, 594, 595, 597, 599, 602, 604, 606, 607, 608, 609, 611, 613, 614, 615, 616, 618, 619, 621, 622, 624, 625, 626, 628, 629, 630, 631, 633, 634, 635, 636, 637, 638, 642, 644, 647, 650, 652, 653, 654, 655, 656, 657, 658, 659, 660, 662, 665, 666, 667, 668, 670, 671, 673, 674, 675, 677, 679, 680, 682, 683, 684, 685, 686, 687, 689, 690, 691, 693, 694, 696, 697, 698, 700, 701, 702, 704, 706, 707, 708, 709, 711, 712, 713, 714, 716, 718, 723, 725, 726, 727, 728, 730, 731, 732, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 747, 748, 749, 751, 752, 753, 755, 757, 758, 759, 761, 763, 764, 765, 766, 767, 768, 769, 771, 772, 773, 774, 776, 778, 779, 781, 782, 783, 784, 787, 788, 790, 791, 792, 794, 795, 796, 797, 799, 800, 801, 802, 806, 807, 809, 810, 811, 812, 814, 817, 818, 819, 820, 821, 822, 824, 827, 829, 830, 831, 832, 833, 834, 835, 837, 838, 845, 846, 847, 848, 849, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 866, 867, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 883, 884, 885, 886, 887, 888, 889, 891, 892, 893, 894, 896, 897, 900, 902, 903, 904, 905, 908, 911, 913, 914, 915, 917, 918, 919, 920, 923, 929, 931, 932, 934, 935, 937, 939, 940, 941, 943, 944, 946, 947, 949, 950, 951, 954, 955, 956, 958, 960, 961, 962, 963, 964, 965, 968, 969, 971, 972, 973, 975, 978, 980, 981, 982, 984, 986, 987, 989, 990, 991, 993, 994, 995, 996, 997, 999, 1001, 1002, 1003, 1005, 1007, 1008, 1009, 1011, 1012, 1013, 1018, 1022, 1023, 1024, 1026, 1028, 1030, 1031, 1032, 1034, 1035, 1036, 1037, 1039, 1040, 1042, 1045, 1048, 1051, 1055, 1056, 1058, 1059, 1061, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1080, 1081, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1091, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1105, 1106, 1107, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1124, 1126, 1127, 1128, 1129, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1150, 1151, 1152, 1156, 1157, 1158, 1159, 1163, 1164, 1165, 1166, 1167, 1169, 1170, 1176, 1177, 1178, 1180, 1181, 1184, 1185, 1187, 1188, 1189, 1190, 1191, 1194, 1195, 1196, 1197, 1199, 1202, 1203, 1206, 1207, 1210, 1211, 1213, 1215, 1216, 1218, 1219, 1220, 1222, 1223, 1224, 1226, 1228, 1229, 1230, 1231, 1232, 1234, 1235, 1236, 1237, 1238, 1240, 1241, 1242, 1243, 1248, 1251, 1252, 1254, 1255, 1257, 1259, 1260, 1262, 1263, 1264, 1267, 1268, 1269, 1270, 1273, 1274, 1275, 1276, 1277, 1278, 1281, 1282, 1283, 1284, 1285, 1288, 1289, 1291, 1293, 1294, 1295, 1296, 1299, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1313, 1314, 1317, 1318, 1319, 1320, 1321, 1323, 1324, 1325, 1326, 1327, 1328, 1330, 1331, 1332, 1333, 1334, 1336, 1337, 1338, 1340, 1343, 1344, 1347, 1348, 1349, 1350, 1352, 1353, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1364, 1366, 1367, 1368, 1369, 1370, 1372, 1374, 1375, 1378, 1379, 1384, 1385, 1386, 1387, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1397, 1398, 1400, 1401, 1402, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1414, 1415, 1419, 1420, 1421, 1423, 1425, 1426, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1439, 1440, 1443, 1445, 1446, 1448, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1461, 1463, 1465, 1468, 1469, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1479, 1481, 1482, 1483, 1485, 1486, 1487, 1488, 1491, 1492, 1493, 1495, 1496, 1497, 1498, 1499]\n",
      "[2, 11, 22, 28, 31, 39, 45, 46, 49, 64, 65, 70, 76, 83, 86, 89, 104, 111, 112, 116, 129, 138, 158, 163, 177, 188, 196, 214, 223, 225, 231, 237, 238, 242, 251, 263, 272, 278, 279, 280, 288, 301, 303, 304, 307, 308, 314, 340, 355, 359, 363, 367, 388, 394, 396, 400, 401, 407, 416, 424, 440, 450, 460, 461, 472, 478, 484, 486, 499, 505, 511, 514, 524, 527, 529, 533, 534, 553, 561, 570, 576, 585, 588, 593, 598, 601, 605, 610, 612, 617, 620, 627, 632, 639, 640, 641, 646, 649, 661, 664, 676, 678, 681, 688, 695, 703, 705, 715, 717, 719, 720, 746, 750, 754, 760, 786, 803, 805, 815, 823, 826, 828, 836, 839, 841, 843, 853, 865, 880, 881, 882, 895, 898, 899, 901, 910, 921, 922, 925, 927, 930, 933, 936, 942, 945, 957, 959, 970, 974, 983, 985, 988, 998, 1004, 1014, 1015, 1016, 1019, 1021, 1025, 1029, 1038, 1041, 1046, 1047, 1049, 1052, 1053, 1057, 1060, 1063, 1078, 1092, 1104, 1130, 1139, 1147, 1148, 1153, 1154, 1155, 1161, 1171, 1174, 1175, 1179, 1192, 1200, 1201, 1205, 1208, 1214, 1221, 1225, 1227, 1233, 1245, 1246, 1249, 1250, 1253, 1258, 1272, 1297, 1300, 1316, 1329, 1335, 1345, 1354, 1365, 1371, 1380, 1396, 1403, 1413, 1417, 1418, 1424, 1437, 1441, 1442, 1449, 1464, 1478, 1484, 1489, 1494]\n",
      "[1071, 98, 332, 1486, 667, 66, 1488, 602, 398, 333, 266, 753, 468, 560, 747, 322, 1327, 202, 1473, 594, 320, 1178, 658, 723, 105, 591, 323, 622, 253, 93, 818, 502, 546, 1445, 1386, 1055, 796, 154, 759, 413, 1018, 1348, 207, 563, 91, 656, 1226, 1131, 97, 1411, 52, 133, 857, 1291, 445, 609, 1275, 293, 765, 343, 799, 581, 474, 549, 1088, 1170, 949, 1254, 766, 508, 846, 599, 354, 228, 1164, 476, 608, 1288, 545, 628, 1083, 554, 512, 768, 1235, 958, 579, 6, 1498, 141, 220, 1405, 261, 229, 739, 114, 604, 951, 364, 830, 1195, 387, 30, 526, 1497, 1127, 329, 575, 1350, 335, 860, 915, 146, 392, 1030, 124, 559, 914, 1169, 809, 686, 1374, 864, 573, 10, 289, 1308, 1166, 619, 479, 82, 374, 812, 319, 1407, 961, 1338, 411, 357, 274, 931, 980, 480, 1039, 1499, 62, 368, 1461, 426, 1065, 247, 544, 235, 1446, 59, 203, 873, 60, 161, 404, 1102, 908, 182, 205, 884, 1295, 147, 1120, 128, 1362, 820, 27, 849, 984, 597, 271, 1119, 1224, 501, 1448, 932, 706, 989, 232, 934, 435, 735, 469, 1317, 997, 90, 73, 492, 1421, 848, 875, 16, 1206, 71, 1194, 892, 637, 1241, 63, 861, 386, 1375, 758, 1243, 452, 1302, 1232, 960, 886, 659, 552, 1220, 236, 1487, 1002, 1069, 370, 845, 151, 969, 227, 1098, 902, 1144, 119, 427, 790, 1121, 911, 771, 1269, 1326, 872, 878, 1097, 498, 1304, 1228, 870, 369, 1135, 1356, 437, 752, 174, 1138, 1270, 352, 382, 51, 1294, 821, 1158, 871, 869, 221, 1384, 55, 737, 1040, 1180, 1093, 1273, 1366, 1267, 814, 567, 167, 1353, 206, 102, 1022, 108, 1458, 211, 847, 1439, 1368, 1310, 1026, 1203, 727, 240, 1457, 1185, 660, 1081, 295, 80, 929, 1176, 377, 1112, 1095, 1074, 1378, 496, 691, 800, 500, 1188, 611, 1108, 1143, 652, 19, 1369, 1387, 698, 483, 682, 1428, 1334, 1324, 791, 883, 451, 1024, 702, 1284, 1360, 121, 635, 1216, 856, 92, 954, 543, 1122, 1328, 950, 195, 1331, 725, 964, 156, 473, 1493, 1299, 485, 535, 1431, 1282, 718, 741, 1469, 996, 564, 54, 1061, 894, 866, 1089, 749, 981, 592, 1157, 794, 465, 33, 423, 644, 550, 153, 571, 1156, 713, 405, 224, 282, 1126, 1023, 1392, 96, 110, 1359, 1367, 801, 621, 684, 806, 944, 1032, 779, 1237, 668, 150, 1474, 372, 462, 614, 378, 542, 234, 1465, 342, 403, 32, 265, 1116, 837, 69, 1379, 36, 1281, 1358, 531, 693, 183, 1151, 1337, 189, 1051, 536, 982, 455, 876, 233, 1336, 1190, 20, 671, 624, 694, 1370, 740, 159, 481, 776, 810, 410, 647, 9, 430, 47, 714, 434, 226, 1361, 1391, 731, 726, 390, 1394, 742, 1075, 324, 778, 738, 665, 1234, 1483, 1218, 1268, 1277, 867, 642, 1481, 1117, 193, 192, 115, 199, 744, 130, 956, 1142, 1132, 245, 78, 819, 1115, 923, 456, 276, 1080, 1076, 943, 1238, 149, 946, 904, 680, 1468, 197, 523, 1395, 250, 1084, 312, 1436, 1451, 117, 1163, 490, 987, 1319, 1306, 674, 704, 972, 583, 690, 491, 72, 662, 1423, 1034, 1035, 734, 748, 494, 1124, 634, 1064, 802, 144, 1219, 448, 327, 577, 299, 811, 1492, 336, 482, 1113, 1340, 1109, 885, 1476, 1333, 269, 297, 109, 1013, 126, 566, 217, 613, 565, 487, 1181, 264, 1408, 862, 420, 859, 1278, 1042, 103, 1260, 587, 222, 1463, 122, 155, 666, 1410, 389, 1482, 888, 834, 1283, 995, 1215, 1471, 1496, 358, 1087, 1012, 795, 1223, 259, 1, 513, 1167, 185, 1150, 106, 101, 165, 568, 74, 993, 356, 896, 1118, 292, 879, 131, 58, 351, 833, 488, 1242, 385, 168, 328, 973, 1007, 939, 584, 757, 1146, 994, 23, 630, 1184, 162, 1231, 1429, 558, 1400, 877, 441, 267, 458, 1325, 1433, 1213, 1128, 631, 679, 539, 172, 409, 366, 518, 874, 1435, 1419, 373, 935, 467, 422, 897, 625, 1067, 495, 697, 1477, 1259, 835, 1293, 712, 79, 1145, 1347, 1459, 140, 555, 1140, 1344, 1434, 331, 262, 449, 782, 0, 562, 1285, 1453, 999, 763, 919, 633, 504, 1479, 1196, 190, 1430, 1289, 1352, 701, 1406, 339, 728, 4, 962, 1491, 175, 246, 184, 707, 466, 1274, 181, 991, 475, 254, 187, 1077, 1070, 817, 1485, 42, 918, 15, 903, 858, 457, 143, 41, 218, 443, 137, 317, 417, 3, 281, 436, 683, 582, 854, 736, 284, 37, 429, 1011, 1036, 761, 311, 1222, 889, 171, 414, 1262, 25, 606, 1332, 459, 283, 346, 792, 402, 399, 655, 418, 677, 1129, 787, 1357, 208, 743, 797, 1475, 309, 784, 990, 1101, 537, 1086, 489, 1003, 1390, 827, 316, 1031, 326, 687, 1495, 415, 557, 673, 941, 1401, 1323, 685, 917, 77, 629, 1314, 556, 1257, 1472, 84, 1409, 548, 1425, 1426, 517, 1313, 1236, 179, 1248, 1100, 1105, 381, 349, 1073, 510, 1318, 675, 773, 709, 315, 1229, 212, 607, 1452, 589, 43, 40, 286, 178, 136, 1385, 711, 406, 1414, 689, 540, 17, 670, 1099, 696, 5, 1309, 180, 824, 1255, 1197, 1094, 38, 965, 169, 50, 230, 708, 1349, 380, 978, 829, 975, 216, 113, 285, 1199, 345, 515, 1202, 863, 384, 1402, 134, 200, 654, 35, 1159, 191, 774, 125, 1440, 616, 305, 1048, 268, 1389, 1191, 1303, 937, 145, 1177, 1141, 1107, 772, 12, 68, 751, 1252, 219, 900, 1133, 831, 887, 626, 1091, 419, 1263, 1364, 1372, 107, 470, 454, 574, 425, 1397, 1355, 249, 1321, 447, 428, 947, 1001, 520, 732, 1412, 1307, 700, 270, 781, 1152, 1210, 955, 471, 822, 255, 497, 1320, 204, 657, 44, 755, 1343, 767, 653, 764, 1137, 1415, 838, 891, 24, 1008, 348, 1106, 439, 201, 551, 769, 1296, 1398, 341, 210, 920, 618, 18, 306, 1096, 650, 1404, 855, 1264, 330, 26, 807, 971, 1136, 100, 1189, 1240, 493, 986, 1066, 615, 1455, 361, 194, 1330, 1420, 256, 135, 1207, 67, 344, 638, 239, 832, 173, 940, 730, 1110, 1251, 302, 1456, 347, 446, 1305, 636, 1037, 1085, 1187, 438, 1276, 1068, 595, 463, 783, 532, 170, 968, 277, 258, 905, 572, 75, 431, 186, 99, 509, 788, 1056, 290, 1211, 963, 1045, 1230, 127, 1432, 1134, 1393, 580, 716, 1072, 294, 300, 1059, 1005, 29, 1028, 913, 893, 152, 1165, 132, 503, 1009, 1058, 421, 1454, 528, 325, 118, 365, 538, 379, 397, 371, 506, 164, 1443, 120, 590, 649, 472, 129, 598, 272, 1021, 1139, 826, 1417, 639, 836, 1258, 1052, 1345, 1449, 1464, 401, 83, 641, 688, 561, 499, 970, 486, 1225, 1200, 214, 593, 1038, 70, 280, 116, 424, 1104, 1418, 1049, 1396, 138, 367, 841, 646, 1019, 231, 695, 610, 461, 64, 746, 524, 238, 307, 1025, 927, 1205, 880, 1221, 279, 484, 22, 898, 930, 933, 304, 882, 910, 511, 815, 46, 664, 388, 620, 576, 1053, 717, 301, 1300, 359, 1179, 1201, 1154, 881, 1174, 601, 1365, 1214, 985, 588, 1155, 617, 533, 1016, 163, 314, 355, 922, 242, 225, 158, 1148, 1046, 45, 553, 1489, 111, 1147, 505, 1297, 853, 786, 440, 640, 1371, 1442, 678, 570, 86, 1249, 104, 1233, 605, 805, 76, 1092, 1441, 529, 1015, 1057, 1078, 288, 1354, 1380, 460, 754, 957, 65, 2, 681, 803, 1208, 394, 1329, 11, 1175, 1494, 901, 750, 39, 1413, 988, 1245, 823, 585, 1004, 983, 899, 1403, 1478, 308, 1130, 839, 89, 407, 828, 363, 28, 400, 760, 450, 223, 1246, 661, 843, 632, 177, 612, 1014, 534, 1437, 478, 959, 263, 1153, 998, 1041, 1060, 719, 1484, 49, 1047, 895, 974, 396, 278, 1029, 251, 112, 1335, 31, 720, 676, 945, 1227, 925, 1171, 627, 1192, 188, 196, 1161, 416, 527, 715, 340, 936, 237, 703, 1250, 921, 865, 942, 303, 1424, 1063, 514, 705, 1272, 1316, 1253]\n",
      "1276\n"
     ]
    }
   ],
   "source": [
    "doc_content_list = []\n",
    "f = open('data/corpus/clean_1500.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('data/1500.train.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('data/ind.1500.test.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "print(ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb722885-eaa0-4437-b32f-831daab78dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['119525\\ttrain\\t28', '112157\\ttrain\\t28', '7773\\ttrain\\t14']\n"
     ]
    }
   ],
   "source": [
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('data/shuffle_1500.txt', 'w')\n",
    "f.write(shuffle_doc_name_str[::500])\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/shuffle_1500.txt', 'w')\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()\n",
    "print(shuffle_doc_name_list[::500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a107d7-8434-4685-9e36-bdfbf405df88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analytical',\n",
       " 'populations',\n",
       " 'formulas',\n",
       " 'lower',\n",
       " 'fully',\n",
       " 'suited',\n",
       " 'added',\n",
       " 'intuitive',\n",
       " 'secrecy',\n",
       " 'pm',\n",
       " 'functional',\n",
       " 'final',\n",
       " 'device',\n",
       " 'ctl',\n",
       " 'species',\n",
       " 'pp',\n",
       " 'programme',\n",
       " 'explanations',\n",
       " 'smooth',\n",
       " 'minimize',\n",
       " 'overlay',\n",
       " 'transactions',\n",
       " 'riemannian',\n",
       " 'evaluate',\n",
       " 'encountered',\n",
       " 'tagging',\n",
       " 'public',\n",
       " 'navigation',\n",
       " 'comprehension',\n",
       " 'categories',\n",
       " 'substantial',\n",
       " 'scalable',\n",
       " 'generates',\n",
       " 'intelligence',\n",
       " 'incomplete',\n",
       " 'cyber',\n",
       " 'weight',\n",
       " 'justified',\n",
       " 'dyads',\n",
       " 'responsibility',\n",
       " 'lfloor',\n",
       " 'intra',\n",
       " 'detection',\n",
       " 'fourier',\n",
       " 'viseme',\n",
       " 'devised',\n",
       " 'converging',\n",
       " 'observer',\n",
       " 'details',\n",
       " 'disparity',\n",
       " 'allocated',\n",
       " 'incentive',\n",
       " 'slight',\n",
       " 'due',\n",
       " 'surprisingly',\n",
       " '5x',\n",
       " 'allows',\n",
       " 'median',\n",
       " 'predicting',\n",
       " 'drift',\n",
       " 'states',\n",
       " 'nonconvex',\n",
       " 'discovered',\n",
       " 'rotation',\n",
       " 'illustrate',\n",
       " 'lifted',\n",
       " 'cost',\n",
       " 'smallest',\n",
       " 'instances',\n",
       " 'enforce',\n",
       " 'quantitatively',\n",
       " 'separable',\n",
       " 'stochasticity',\n",
       " 'reducing',\n",
       " 'interactive',\n",
       " 'distortions',\n",
       " 'city',\n",
       " 'tml',\n",
       " 'recurrent',\n",
       " 'procedural',\n",
       " 'mild',\n",
       " 'lattice',\n",
       " 'produce',\n",
       " 'computing',\n",
       " 'means',\n",
       " 'despite',\n",
       " 'region',\n",
       " 'compared',\n",
       " 'methodologies',\n",
       " 'inside',\n",
       " 'unions',\n",
       " 'taken',\n",
       " 'expression',\n",
       " 'reasons',\n",
       " 'preceding',\n",
       " 'report',\n",
       " 'goes',\n",
       " 'often',\n",
       " 'uncertainty',\n",
       " 'arms',\n",
       " 'comparing',\n",
       " 'energy',\n",
       " 'formalized',\n",
       " 'els',\n",
       " 'spreadsheets',\n",
       " 'computation',\n",
       " 'fewer',\n",
       " 'pc',\n",
       " 'learner',\n",
       " 'cohesive',\n",
       " 'assessment',\n",
       " 'decoded',\n",
       " 'tutorial',\n",
       " 'pca',\n",
       " 'receiver',\n",
       " 'tomography',\n",
       " 'sprites',\n",
       " 'render',\n",
       " 'counts',\n",
       " 'vs',\n",
       " 'bed',\n",
       " 'wise',\n",
       " 'desirable',\n",
       " 'goals',\n",
       " 'demonstrated',\n",
       " 'task',\n",
       " 'arrays',\n",
       " 'reaches',\n",
       " 'gaze',\n",
       " 'antenna',\n",
       " 'coding',\n",
       " 'longer',\n",
       " 'faces',\n",
       " 'algebras',\n",
       " 'arbitrarily',\n",
       " 'resilience',\n",
       " 'click',\n",
       " 'suggests',\n",
       " 'forgetting',\n",
       " 'systematically',\n",
       " 'relative',\n",
       " 'tion',\n",
       " 'https',\n",
       " 'localize',\n",
       " 'advice',\n",
       " 'extract',\n",
       " 'extremely',\n",
       " 'ultimately',\n",
       " 'lg',\n",
       " 'adcs',\n",
       " 'hyperparameters',\n",
       " 'sea',\n",
       " 'bob',\n",
       " 'limits',\n",
       " 'circumvent',\n",
       " 'clearly',\n",
       " 'arrival',\n",
       " 'know',\n",
       " 'validated',\n",
       " 'sat',\n",
       " 'reformulate',\n",
       " 'tools',\n",
       " 'parameterised',\n",
       " 'factor',\n",
       " 'size',\n",
       " 'cause',\n",
       " 'differently',\n",
       " 'tracking',\n",
       " 'shot',\n",
       " 'ideal',\n",
       " 'u',\n",
       " 'biological',\n",
       " 'disease',\n",
       " 'access',\n",
       " 'asr',\n",
       " 'agnostic',\n",
       " 'modifications',\n",
       " 'contribute',\n",
       " 'firstly',\n",
       " 'layered',\n",
       " 'somewhat',\n",
       " 'evaluated',\n",
       " 'reduces',\n",
       " 'hypotheses',\n",
       " 'attributed',\n",
       " 'detc',\n",
       " 'spatial',\n",
       " '96',\n",
       " 'ldpc',\n",
       " 'improves',\n",
       " 'matcher',\n",
       " 'successive',\n",
       " 'ordering',\n",
       " 'peg',\n",
       " 'freely',\n",
       " 'tour',\n",
       " 'maximizes',\n",
       " 'addition',\n",
       " 'peak',\n",
       " 'receives',\n",
       " 'rarely',\n",
       " 'oblivious',\n",
       " 'honest',\n",
       " 'contention',\n",
       " 'odometry',\n",
       " 'atari',\n",
       " 'management',\n",
       " 'imitation',\n",
       " 'discovery',\n",
       " 'varying',\n",
       " 'motions',\n",
       " 'group',\n",
       " 'node',\n",
       " 'reduction',\n",
       " 'competing',\n",
       " 'ergodic',\n",
       " 'velocity',\n",
       " 'comprise',\n",
       " 'items',\n",
       " 'allowed',\n",
       " 'flows',\n",
       " 'anomalies',\n",
       " 'scalar',\n",
       " 'effectiveness',\n",
       " 'say',\n",
       " 'reflectance',\n",
       " 'nlvr2',\n",
       " 'allocation',\n",
       " 'reciprocity',\n",
       " 'relaxed',\n",
       " 'subjects',\n",
       " 'aggregate',\n",
       " 'planner',\n",
       " 'semantically',\n",
       " 'entities',\n",
       " 'local',\n",
       " 'complicated',\n",
       " 'flow',\n",
       " 'devise',\n",
       " 'bilingual',\n",
       " 'modulation',\n",
       " 'constitutive',\n",
       " 'thorough',\n",
       " 'sheds',\n",
       " 'intended',\n",
       " 'representable',\n",
       " 'smoothing',\n",
       " 'targeting',\n",
       " 'abrupt',\n",
       " 'blocks',\n",
       " 'negatively',\n",
       " 'breast',\n",
       " 'automate',\n",
       " 'sourced',\n",
       " 'modification',\n",
       " 'scalability',\n",
       " 'classical',\n",
       " 'lte',\n",
       " 'counterparts',\n",
       " 'mpc',\n",
       " 'significant',\n",
       " 'default',\n",
       " 'crnn',\n",
       " 'gaps',\n",
       " 'twitter',\n",
       " 'usual',\n",
       " 'adjacency',\n",
       " 'modular',\n",
       " 'clock',\n",
       " 'presented',\n",
       " 'across',\n",
       " 'embedding',\n",
       " 'satellite',\n",
       " '15',\n",
       " 'core',\n",
       " 'setup',\n",
       " 'pipeline',\n",
       " 'regimes',\n",
       " 'revealed',\n",
       " 'heavily',\n",
       " 'unbiased',\n",
       " 'canonical',\n",
       " 'areas',\n",
       " 'classes',\n",
       " 'integrators',\n",
       " 'drones',\n",
       " 'effort',\n",
       " 'twofold',\n",
       " 'observable',\n",
       " 'satisfied',\n",
       " 'particle',\n",
       " 'obtaining',\n",
       " 'planarity',\n",
       " 'docred',\n",
       " 'specification',\n",
       " 'brings',\n",
       " 'appropriate',\n",
       " 'decreasing',\n",
       " 'android',\n",
       " 'f1',\n",
       " 'dictionary',\n",
       " 'context',\n",
       " 'takes',\n",
       " 'uses',\n",
       " 'leakage',\n",
       " 'away',\n",
       " 'universal',\n",
       " 'select',\n",
       " 'complexities',\n",
       " 'limitation',\n",
       " 'recognized',\n",
       " 'additive',\n",
       " 'calculate',\n",
       " 'see',\n",
       " 'formalization',\n",
       " 'current',\n",
       " 'solvers',\n",
       " 'temperature',\n",
       " 'action',\n",
       " 'neighbor',\n",
       " 'scaling',\n",
       " 'break',\n",
       " 'regularizer',\n",
       " '1d',\n",
       " 'variables',\n",
       " 'sketch',\n",
       " 'follows',\n",
       " 'trajectory',\n",
       " 'implications',\n",
       " 'enhancing',\n",
       " 'byzantine',\n",
       " '98',\n",
       " 'emptyheaded',\n",
       " 'extractive',\n",
       " 'motivates',\n",
       " 'positioning',\n",
       " 'temporal',\n",
       " 'organization',\n",
       " 'construct',\n",
       " 'tend',\n",
       " 'mitigate',\n",
       " 'readily',\n",
       " 'modality',\n",
       " 'communications',\n",
       " 'ordinary',\n",
       " 'provably',\n",
       " 'encoder',\n",
       " 'intermediate',\n",
       " 'accelerate',\n",
       " 'equivalent',\n",
       " 'follownet',\n",
       " 'randomly',\n",
       " 'checking',\n",
       " 'mimo',\n",
       " 'kws',\n",
       " 'reliable',\n",
       " 'indices',\n",
       " 'adversarially',\n",
       " 'outperforms',\n",
       " 'solve',\n",
       " 'lhe',\n",
       " 'transmitting',\n",
       " 'similar',\n",
       " 'uplink',\n",
       " 'radiology',\n",
       " 'chemical',\n",
       " 'ad',\n",
       " 'letters',\n",
       " 'monte',\n",
       " 'conclusions',\n",
       " 'difficulty',\n",
       " 'plane',\n",
       " 'aggressive',\n",
       " 'robotic',\n",
       " 'university',\n",
       " 'transferring',\n",
       " 'attempts',\n",
       " 'objects',\n",
       " 'apps',\n",
       " 'weisfeiler',\n",
       " 'armed',\n",
       " 'key',\n",
       " 'lightweight',\n",
       " 'barrier',\n",
       " 'qos',\n",
       " 'routine',\n",
       " 'hypergraph',\n",
       " 'mainly',\n",
       " 'game',\n",
       " 'controlling',\n",
       " 'released',\n",
       " 'conversations',\n",
       " 'viewlet',\n",
       " 'ber',\n",
       " 'includes',\n",
       " 'xr',\n",
       " 'tgicp',\n",
       " 'manipulations',\n",
       " 'oba',\n",
       " 'anarchy',\n",
       " 'constructs',\n",
       " 'sharing',\n",
       " 'mapping',\n",
       " 'popularity',\n",
       " 'sql',\n",
       " 'payment',\n",
       " 'rely',\n",
       " 'fisher',\n",
       " 'technologies',\n",
       " 'events',\n",
       " 'elps',\n",
       " 'tremendous',\n",
       " 'test',\n",
       " 'earlier',\n",
       " 'alternatives',\n",
       " 'suitability',\n",
       " 'define',\n",
       " 'successful',\n",
       " 'degree',\n",
       " 'heuristics',\n",
       " 'perspectives',\n",
       " 'issues',\n",
       " 'learn',\n",
       " 'calculating',\n",
       " 'sequential',\n",
       " 'behind',\n",
       " 'needed',\n",
       " 'sublinear',\n",
       " 'facilities',\n",
       " 'adds',\n",
       " 'mst',\n",
       " 'problem',\n",
       " 'almost',\n",
       " 'english',\n",
       " 'novelty',\n",
       " 'priors',\n",
       " 'former',\n",
       " 'gains',\n",
       " 'letter',\n",
       " 'affected',\n",
       " 'analysis',\n",
       " 'beta',\n",
       " 'degraded',\n",
       " 'nonlinear',\n",
       " 'inference',\n",
       " 'second',\n",
       " 'brief',\n",
       " 'neurons',\n",
       " 'distribution',\n",
       " 'db',\n",
       " 'asset',\n",
       " 'plurality',\n",
       " 'characterizing',\n",
       " 'constructed',\n",
       " 'conclude',\n",
       " 'extra',\n",
       " 'encryption',\n",
       " 'generator',\n",
       " 'pixel',\n",
       " 'original',\n",
       " 'handle',\n",
       " 'triangulation',\n",
       " 'computed',\n",
       " 'http',\n",
       " 'mentioned',\n",
       " 'hummingbird',\n",
       " 'af',\n",
       " 'voting',\n",
       " 'pivot',\n",
       " 'indexes',\n",
       " '93',\n",
       " 'clouds',\n",
       " 'ptime',\n",
       " 'mathbb',\n",
       " 'viewed',\n",
       " 'actually',\n",
       " 'sliding',\n",
       " 'preparation',\n",
       " 'assistant',\n",
       " 'usage',\n",
       " 'adapted',\n",
       " 'bipedal',\n",
       " 'dominating',\n",
       " 'paths',\n",
       " 'parameterized',\n",
       " 'hsi',\n",
       " 'prior',\n",
       " 'condition',\n",
       " 'digital',\n",
       " 'formulated',\n",
       " 'deploying',\n",
       " 'domains',\n",
       " 'tables',\n",
       " 'disjoint',\n",
       " 'rf',\n",
       " 'negative',\n",
       " 'central',\n",
       " 'regarding',\n",
       " 'showed',\n",
       " 'submp',\n",
       " 'meets',\n",
       " 'keywords',\n",
       " 'prevent',\n",
       " 'hop',\n",
       " 'simulate',\n",
       " 'burning',\n",
       " '2010',\n",
       " 'nvidia',\n",
       " 'scaled',\n",
       " 'far',\n",
       " 'preferences',\n",
       " 'uniqueness',\n",
       " 'according',\n",
       " 'known',\n",
       " 'heuristic',\n",
       " 'yet',\n",
       " 'examples',\n",
       " 'compact',\n",
       " 'employs',\n",
       " 'well',\n",
       " 'assimilation',\n",
       " 'subsequently',\n",
       " 'perspective',\n",
       " 'inherently',\n",
       " 'characterization',\n",
       " 'kinds',\n",
       " 'represent',\n",
       " 'adding',\n",
       " 'mmse',\n",
       " 'parameter',\n",
       " 'discussion',\n",
       " 'force',\n",
       " 'geo',\n",
       " 'ipa',\n",
       " 'widely',\n",
       " 'yes',\n",
       " 'ctc',\n",
       " 'amounts',\n",
       " 'although',\n",
       " 'advantages',\n",
       " 'fuzzing',\n",
       " 'presence',\n",
       " 'theoretically',\n",
       " 'outperform',\n",
       " 'paid',\n",
       " 'status',\n",
       " 'smp',\n",
       " 'estimation',\n",
       " 'markov',\n",
       " 'pieces',\n",
       " 'hypergraphs',\n",
       " 'maximized',\n",
       " 'either',\n",
       " 'lbp',\n",
       " 'black',\n",
       " 'series',\n",
       " 'subspace',\n",
       " 'peer',\n",
       " 'resultant',\n",
       " 'fracture',\n",
       " 'called',\n",
       " 'latter',\n",
       " 'external',\n",
       " 'pearl',\n",
       " 'hr',\n",
       " 'understand',\n",
       " 'kalman',\n",
       " 'person',\n",
       " 'monocular',\n",
       " 'direction',\n",
       " 'double',\n",
       " 'alice',\n",
       " 'factorization',\n",
       " 'upper',\n",
       " 'distortion',\n",
       " 'method',\n",
       " 'linked',\n",
       " 'however',\n",
       " 'cohort',\n",
       " 'dichotomy',\n",
       " 'aiming',\n",
       " 'match',\n",
       " 'frame',\n",
       " 'host',\n",
       " 'productivity',\n",
       " 'join',\n",
       " 'laws',\n",
       " 'normalization',\n",
       " 'counterpart',\n",
       " 'spectrum',\n",
       " 'influence',\n",
       " 'approximately',\n",
       " 'nist',\n",
       " 'learners',\n",
       " 'tolerance',\n",
       " 'policies',\n",
       " 'variety',\n",
       " 'input',\n",
       " 'reveal',\n",
       " 'friendly',\n",
       " 'entity',\n",
       " 'careful',\n",
       " 'aggregation',\n",
       " 'granularity',\n",
       " 'none',\n",
       " 'predictable',\n",
       " 'prolog',\n",
       " 'tau',\n",
       " 'summary',\n",
       " 'transformed',\n",
       " 'extensively',\n",
       " 'practice',\n",
       " 'belong',\n",
       " 'question',\n",
       " 'tracks',\n",
       " 'dnn',\n",
       " 'crafted',\n",
       " 'ensures',\n",
       " 'analysts',\n",
       " 'postulates',\n",
       " 'inversion',\n",
       " 'estimates',\n",
       " 'vanishing',\n",
       " 'knowledge',\n",
       " 'dl',\n",
       " 'meanwhile',\n",
       " 'emphasis',\n",
       " 'first',\n",
       " 'contain',\n",
       " 'impressive',\n",
       " 'lastly',\n",
       " 'would',\n",
       " 'grids',\n",
       " 'bch',\n",
       " 'consider',\n",
       " 'point',\n",
       " 'wide',\n",
       " 'adopt',\n",
       " 'fact',\n",
       " 'petri',\n",
       " 'updating',\n",
       " 'comparative',\n",
       " 'algebraic',\n",
       " 'training',\n",
       " 'answers',\n",
       " 'show',\n",
       " 'improved',\n",
       " 'classify',\n",
       " 'df',\n",
       " 'singularity',\n",
       " 'together',\n",
       " 'suitable',\n",
       " 'lat',\n",
       " 'updates',\n",
       " 'ip',\n",
       " 'drl',\n",
       " 'knapsack',\n",
       " 'apply',\n",
       " 'affects',\n",
       " 'ai',\n",
       " 'retrieve',\n",
       " 'hard',\n",
       " 'types',\n",
       " 'interpretation',\n",
       " 'implementation',\n",
       " 'hours',\n",
       " 'primary',\n",
       " 'relevance',\n",
       " 'graphs',\n",
       " 'collective',\n",
       " 'indicating',\n",
       " 'concurrent',\n",
       " 'page',\n",
       " '77',\n",
       " 'sacrificing',\n",
       " 'robotics',\n",
       " 'optimal',\n",
       " 'insider',\n",
       " 'abstract',\n",
       " 'mean',\n",
       " '100',\n",
       " 'photometric',\n",
       " 'hyperspectral',\n",
       " 'assistance',\n",
       " 'remote',\n",
       " 'compatible',\n",
       " 'textit',\n",
       " 'budget',\n",
       " 'corpus',\n",
       " 'classified',\n",
       " 'datasets',\n",
       " 'law',\n",
       " 'analyse',\n",
       " 'capturing',\n",
       " 'exhibiting',\n",
       " 'combat',\n",
       " 'deepsdf',\n",
       " 'trying',\n",
       " 'sf',\n",
       " 'enough',\n",
       " 'satisfactory',\n",
       " 'bus',\n",
       " 'cues',\n",
       " 'virtually',\n",
       " 'quality',\n",
       " 'multitude',\n",
       " 'posteriori',\n",
       " 'notions',\n",
       " 'ensembles',\n",
       " 'bw',\n",
       " 'internal',\n",
       " 'insensitive',\n",
       " 'multiplication',\n",
       " 'facto',\n",
       " 'rfloor',\n",
       " 'rich',\n",
       " 'planning',\n",
       " 'interacting',\n",
       " 'template',\n",
       " '8x',\n",
       " 'prove',\n",
       " 'goal',\n",
       " 'truthful',\n",
       " 'colorization',\n",
       " 'modeled',\n",
       " 'equation',\n",
       " 'topological',\n",
       " 'decoder',\n",
       " 'dialog',\n",
       " 'insufficient',\n",
       " 'transmitters',\n",
       " 'cooled',\n",
       " 'platforms',\n",
       " 'autonomous',\n",
       " 'exploit',\n",
       " 'obtains',\n",
       " 'blind',\n",
       " 'treated',\n",
       " 'travel',\n",
       " 'thought',\n",
       " 'attributes',\n",
       " 'manifest',\n",
       " 'describe',\n",
       " 'help',\n",
       " 'harvester',\n",
       " 'sleep',\n",
       " 'candidate',\n",
       " 'shed',\n",
       " 'logs',\n",
       " 'workspace',\n",
       " 'replication',\n",
       " 'outside',\n",
       " 'discussed',\n",
       " 'closest',\n",
       " 'placement',\n",
       " 'preference',\n",
       " 'labelled',\n",
       " 'worker',\n",
       " 'keys',\n",
       " 'communicating',\n",
       " 'fm',\n",
       " 'zero',\n",
       " 'navigating',\n",
       " 'theoretical',\n",
       " 'ice',\n",
       " 'social',\n",
       " 'capture',\n",
       " 'correlated',\n",
       " 'negligible',\n",
       " 'corpora',\n",
       " 'embed',\n",
       " 'examines',\n",
       " 'anonymization',\n",
       " 'tighter',\n",
       " 'obvious',\n",
       " 'uav',\n",
       " 'cope',\n",
       " 'fifth',\n",
       " 'conformant',\n",
       " 'flight',\n",
       " 'problems',\n",
       " 'conditions',\n",
       " 'ofdm',\n",
       " 'rental',\n",
       " 'f',\n",
       " 'boundary',\n",
       " 'generating',\n",
       " 'batch',\n",
       " 'relays',\n",
       " 'measurements',\n",
       " 'representing',\n",
       " 'adopting',\n",
       " 'contributions',\n",
       " 'shows',\n",
       " 'drum',\n",
       " 'stage',\n",
       " 'secret',\n",
       " 'concerns',\n",
       " 'expectation',\n",
       " 'fo',\n",
       " 'attain',\n",
       " 'messages',\n",
       " 'understanding',\n",
       " 'seek',\n",
       " 'mimic',\n",
       " 'written',\n",
       " 'compensate',\n",
       " 'encode',\n",
       " 'steiner',\n",
       " 'meshes',\n",
       " 'decentralized',\n",
       " 'categorization',\n",
       " 'surfaces',\n",
       " 'quadratic',\n",
       " 'extensions',\n",
       " 'specified',\n",
       " 'ni',\n",
       " 'collateral',\n",
       " 'synchronized',\n",
       " 'assuming',\n",
       " 'null',\n",
       " 'anatomical',\n",
       " 'existing',\n",
       " 'investigated',\n",
       " 'nontrivial',\n",
       " 'definitions',\n",
       " 'communicate',\n",
       " 'annotation',\n",
       " 'processes',\n",
       " 'optimization',\n",
       " 'differentially',\n",
       " 'carlo',\n",
       " 'idea',\n",
       " 'encompasses',\n",
       " 'employed',\n",
       " 'characters',\n",
       " 'discriminative',\n",
       " 'circuit',\n",
       " 'rounding',\n",
       " 'showcase',\n",
       " 'ratio',\n",
       " 'dimensions',\n",
       " 'addresses',\n",
       " 'formed',\n",
       " 'satisfy',\n",
       " 'max',\n",
       " 'measurement',\n",
       " 'thanks',\n",
       " 'currently',\n",
       " 'nested',\n",
       " 'discover',\n",
       " 'assist',\n",
       " 'aim',\n",
       " 'mcs',\n",
       " 'medical',\n",
       " 'earthquake',\n",
       " 'structures',\n",
       " 'activations',\n",
       " 'remaining',\n",
       " 'libraries',\n",
       " 'randomness',\n",
       " 'several',\n",
       " 'nmt',\n",
       " 'robustness',\n",
       " 'workflows',\n",
       " 'homomorphic',\n",
       " 'primitive',\n",
       " 'grouping',\n",
       " 'exact',\n",
       " 'relationships',\n",
       " 'mask',\n",
       " 'perform',\n",
       " 'comments',\n",
       " 'investigation',\n",
       " 'phi',\n",
       " 'held',\n",
       " 'indicator',\n",
       " 'knowing',\n",
       " 'might',\n",
       " 'equipped',\n",
       " 'frac',\n",
       " 'noma',\n",
       " 'fortran',\n",
       " '\\\\(',\n",
       " 'modal',\n",
       " 'java',\n",
       " 'tradeoff',\n",
       " 'snns',\n",
       " 'scenes',\n",
       " 'rfa',\n",
       " 'commodity',\n",
       " 'logical',\n",
       " 'team',\n",
       " 'motion',\n",
       " 'maximization',\n",
       " 'identifying',\n",
       " 'millimeter',\n",
       " 'represented',\n",
       " 'eigenvalues',\n",
       " 'never',\n",
       " 'camera',\n",
       " 'vlc',\n",
       " '2014',\n",
       " 'alternating',\n",
       " 'strategies',\n",
       " 'adapt',\n",
       " 'regions',\n",
       " 'color',\n",
       " 'robots',\n",
       " 'purposes',\n",
       " 'explain',\n",
       " 'joins',\n",
       " 'github',\n",
       " 'correlate',\n",
       " 'strict',\n",
       " 'companies',\n",
       " 'classifiers',\n",
       " 'virtualization',\n",
       " 'train',\n",
       " '39',\n",
       " 'requirements',\n",
       " 'refinement',\n",
       " 'association',\n",
       " 'equations',\n",
       " 'gold',\n",
       " 'propagation',\n",
       " 'controllable',\n",
       " 'reliably',\n",
       " 'v1',\n",
       " 'recommender',\n",
       " 'frequent',\n",
       " 'incorporates',\n",
       " 'list',\n",
       " 'softnull',\n",
       " 'launch',\n",
       " 'modelled',\n",
       " 'bellwether',\n",
       " 'hiring',\n",
       " 'outline',\n",
       " 'estimators',\n",
       " '79',\n",
       " 'amt',\n",
       " 'transpositions',\n",
       " 'describing',\n",
       " 'estimated',\n",
       " 'departments',\n",
       " 'referred',\n",
       " 'relying',\n",
       " 'tests',\n",
       " 'counter',\n",
       " 'outperformed',\n",
       " 'alphabet',\n",
       " 'comprising',\n",
       " 'integrals',\n",
       " 'achievable',\n",
       " 'two',\n",
       " 'sensing',\n",
       " 'unfortunately',\n",
       " 'protection',\n",
       " '9',\n",
       " 'terminal',\n",
       " 'record',\n",
       " 'boosted',\n",
       " 'efx',\n",
       " 'pgcr',\n",
       " 'implies',\n",
       " 'agreement',\n",
       " 'smart',\n",
       " 'overview',\n",
       " 'statistics',\n",
       " 'prime',\n",
       " 'grammars',\n",
       " 'graphic',\n",
       " 'exploitation',\n",
       " 'nesterov',\n",
       " 'su',\n",
       " 'showing',\n",
       " 'soft',\n",
       " 'originally',\n",
       " 'clear',\n",
       " '10',\n",
       " 'trainable',\n",
       " 'processed',\n",
       " 'timely',\n",
       " 'parabetatail',\n",
       " 'failures',\n",
       " 'target',\n",
       " 'rise',\n",
       " 'unstructured',\n",
       " 'notably',\n",
       " 'conduct',\n",
       " 'introducing',\n",
       " 'dynamically',\n",
       " 'pool',\n",
       " 'imbalanced',\n",
       " 'markers',\n",
       " 'indoor',\n",
       " 'interleaved',\n",
       " 'consistently',\n",
       " 'complemented',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb13dc6-26d9-4c85-b345-7660074fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "f = open('data/corpus/vocab_1500.txt', 'w')\n",
    "f.write(vocab_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cb7fb6-0cac-4415-a0ec-9bb585c1a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "f = open('data/corpus/labels_1500.txt', 'w')\n",
    "f.write(label_list_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8adda60-fea3-4d0f-a8b9-8103d745a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "# different training rates\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "f = open('data/real_train_1500.name', 'w')\n",
    "f.write(real_train_doc_names_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a762e0e8-4e33-46ea-8b80-236d8c820b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "print(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae35e3cb-a117-45be-bc6b-1a068fd9aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(944, 300) (944, 38) (228, 300) (228, 38) (5780, 300) (5780, 38)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b9cabe-e372-414d-8d40-72c0c5e46c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4b9f9c-3cef-465f-bb13-e3b1037a431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# word vector cosine similarity as weights\n",
    "\n",
    "'''\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
    "            vector_i = np.array(word_vector_map[vocab[i]])\n",
    "            vector_j = np.array(word_vector_map[vocab[j]])\n",
    "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
    "            if similarity > 0.9:\n",
    "                print(vocab[i], vocab[j], similarity)\n",
    "                row.append(train_size + i)\n",
    "                col.append(train_size + j)\n",
    "                weight.append(similarity)\n",
    "'''\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f74fc5-511a-4600-8cf8-45836dafce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dump objects\n",
    "f = open(\"data/ind.x\", 'wb')\n",
    "pkl.dump(x, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.y\", 'wb')\n",
    "pkl.dump(y, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.tx\", 'wb')\n",
    "pkl.dump(tx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.ty\", 'wb')\n",
    "pkl.dump(ty, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.allx\", 'wb')\n",
    "pkl.dump(allx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.ally\", 'wb')\n",
    "pkl.dump(ally, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.adj\", 'wb')\n",
    "pkl.dump(adj, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d34d9-86da-47c9-b9fa-71d550301beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
