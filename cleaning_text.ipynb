{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c35369-2185-4711-9244-7fd1d2126bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9485079f-5bb9-4164-9c0e-e25181ffed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'or', 'below', 'here', \"doesn't\", 'once', 'has', 'were', 'what', 's', \"didn't\", \"shan't\", 'who', 'your', 'o', 'herself', 'from', 'shan', \"hadn't\", 'he', 'doesn', \"you're\", 'how', \"haven't\", 't', 'we', 'isn', 'just', \"weren't\", \"that'll\", 'do', 'hers', 'our', 'own', 'only', 'wouldn', 'few', \"don't\", 'over', 'y', 'further', 'so', \"should've\", 'didn', 'very', 'aren', 'but', 'have', 'be', 'don', 'such', \"wouldn't\", 'about', 'a', 'did', 'd', 'in', 'myself', 'its', 'does', 'same', 'then', 'other', \"it's\", 'until', 'them', \"you'll\", 'after', 'yourself', 'through', 'couldn', 'they', 'll', 'when', 'again', \"needn't\", 'won', 'doing', 'him', \"you've\", 'both', 'off', 'at', 'his', 'against', 'under', 'no', 'because', 'as', \"mightn't\", 'i', 'the', 'there', \"mustn't\", 'her', 'of', 'on', 'why', 'this', 'had', 'whom', 'should', 'hasn', \"you'd\", 'these', 'nor', \"wasn't\", 'with', 'out', \"won't\", 'for', 're', 'it', \"aren't\", \"couldn't\", 'hadn', 'above', 'needn', 'that', 'most', 'ain', 'before', 'you', 'if', 'having', 'into', 'each', 'haven', 'weren', 'while', 'shouldn', 'yourselves', 'which', 'can', 'been', 'during', 'than', \"shouldn't\", 'up', 'she', 'theirs', 'itself', 'ourselves', 'was', 'some', 've', 'being', 'an', 'themselves', 'not', 'my', 'mustn', 'am', 'ma', 'all', 'is', 'where', 'those', 'wasn', 'between', 'more', 'too', \"she's\", 'by', 'me', 'to', 'will', 'are', 'now', 'm', \"hasn't\", 'and', 'yours', 'himself', 'down', 'ours', 'any', \"isn't\", 'mightn', 'their'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sjain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from utils import clean_str, loadWord2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2867ca0-a107-4b58-83b7-b1c77a6d9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110224, 146930, 2941, 104545, 62327, 29760, 96891, 47026, 117733, 163207, 61451, 20590, 145423, 33883, 4524, 81255, 82144, 85139, 167094, 125904, 116418, 158098, 95233, 81836, 84071, 53850, 112264, 17238, 34056, 10708, 164819, 32413, 75065, 139894, 161233, 100688, 148030, 55853, 23077, 152167, 44378, 111683, 145091, 132167, 83409, 153865, 143858, 111283, 150657, 36240, 142480, 112754, 149469, 50328, 163801, 45304, 155756, 151190, 160715, 8876, 116827, 142384, 143574, 136796, 278, 120989, 120991, 151642, 44083, 94917, 32815, 117614, 98217, 41937, 16626, 163885, 69261, 29037, 91370, 31452, 161506, 27949, 29236, 15855, 162541, 161900, 157800, 90014, 120276, 57310, 92875, 140990, 11693, 78368, 62286, 80503, 72673, 121791, 12663, 90678]\n",
      "['construction of q ary constant weight sequences using a knuth like approach We present an encoding and decoding scheme for constant weight sequences, that is, given an information sequence, the construction results in a sequence of specific weight within a certain range. The scheme uses a prefix design that is based on Gray codes. Furthermore, by adding redundant symbols we extend the range of weight values for output sequences, which is useful for some applications.', 'egalitarian deliberative decision making We study a scenario in which an agent population wishes to identify a majority-supported proposal to change the status quo, where proposals are elements of an abstract space. In our deliberation process agents form coalitions around proposals that they prefer over the status quo, and can then join forces to create larger coalitions by merging two coalitions around a new proposal, possibly leaving dissenting agents behind. We identify a class of metric spaces, which includes all Euclidean spaces, for which the deliberation process is guaranteed to result in \\\\emph{success}: Deliberation will terminate with a majority coalition, as long as a majority-supported proposal exists. We then consider a more general proposal space where deliberation might not be successful. For such a setting we study sufficient conditions on the type of deliberation that can again guarantee success. We complement our analysis with a centralized dynamic programming algorithm to identify the most-supported proposal.', 'decentralized dynamic optimization for power network voltage control Voltage control in power distribution networks has been greatly challenged by the increasing penetration of volatile and intermittent devices. These devices can also provide limited reactive power resources that can be used to regulate the network-wide voltage. A decentralized voltage control strategy can be designed by minimizing a quadratic voltage mismatch error objective using gradient-projection (GP) updates. Coupled with the power network flow, the local voltage can provide the instantaneous gradient information. This paper aims to analyze the performance of this decentralized GP-based voltage control design under two dynamic scenarios: i) the nodes perform the decentralized update in an asynchronous fashion, and ii) the network operating condition is time-varying. For the asynchronous voltage control, we improve the existing convergence condition by recognizing that the voltage based gradient is always up-to-date. By modeling the network dynamics using an autoregressive process and considering time-varying resource constraints, we provide an error bound in tracking the instantaneous optimal solution to the quadratic error objective. This result can be extended to more general \\\\textit{constrained dynamic optimization} problems with smooth strongly convex objective functions under stochastic processes that have bounded iterative changes. Extensive numerical tests have been performed to demonstrate and validate our analytical results for realistic power networks.', 'integer reset timed automata clock reduction and determinizability In this paper, we propose a procedure that given an integer reset timed automaton (IRTA) ${\\\\cal A}$, produces a language equivalent deterministic one clock IRTA ${\\\\cal B}$ whose size is at most doubly exponential in the size of ${\\\\cal A}$. We prove that this bound on the number of locations is tight. Further, if integer resets are used in stopwatch automata, a subclass of stopwatch automata which is closed under all boolean operations and for which reachability is decidable is obtained.', 'repair rate lower bounds for distributed storage One of the primary objectives of a distributed storage system is to reliably store a large amount $dsize$ of source data for a long duration using a large number $N$ of unreliable storage nodes, each with capacity $nsize$. The storage overhead $\\\\beta$ is the fraction of system capacity available beyond $dsize$, i.e., $\\\\beta = 1- \\\\frac{dsize}{N \\\\cdot nsize}$. Storage nodes fail randomly over time and are replaced with initially empty nodes, and thus data is erased from the system at an average rate $erate = \\\\lambda \\\\cdot N \\\\cdot nsize$, where $1/\\\\lambda$ is the average lifetime of a node before failure. To maintain recoverability of the source data, a repairer continually reads data over a network from nodes at some average rate $rrate$, and generates and writes data to nodes based on the read data. The main result is that, for any repairer, if the source data is recoverable at each point in time then it must be the case that $rrate \\\\ge \\\\frac{erate}{2 \\\\cdot \\\\beta}$ asymptotically as $N$ goes to infinity and beta goes to zero. This inequality provides a fundamental lower bound on the average rate that any repairer needs to read data from the system in order to maintain recoverability of the source data.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/graph_node_corpus.txt', 'rb')\n",
    "# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
    "for line in f.readlines():\n",
    "    doc_content_list.append(line.strip().decode('latin1'))\n",
    "f.close()\n",
    "total_lines = len(doc_content_list)\n",
    "\n",
    "# Create a list of indices (0 to total_lines-1)\n",
    "line_indices = list(range(total_lines))\n",
    "\n",
    "# Randomly select 100 indices from the line indices\n",
    "selected_indices = random.sample(line_indices, min(100, total_lines))\n",
    "\n",
    "# Retrieve the corresponding document contents using the selected indices\n",
    "random_lines = [doc_content_list[i] for i in selected_indices]\n",
    "doc_content_list = random_lines\n",
    "print(selected_indices)\n",
    "print(doc_content_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd2c8612-8988-4f9a-baec-0a42bb305f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glottal',\n",
       " 'closure',\n",
       " 'instants',\n",
       " 'detection',\n",
       " 'from',\n",
       " 'pathological',\n",
       " 'acoustic',\n",
       " 'speech',\n",
       " 'signal',\n",
       " 'using',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " ',',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'classification',\n",
       " 'based',\n",
       " 'glottal',\n",
       " 'closure',\n",
       " 'instants',\n",
       " '\\\\(',\n",
       " 'gci',\n",
       " '\\\\)',\n",
       " 'detection',\n",
       " 'from',\n",
       " 'pathological',\n",
       " 'acoustic',\n",
       " 'speech',\n",
       " 'signal',\n",
       " ',',\n",
       " 'which',\n",
       " 'finds',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'vocal',\n",
       " 'disorder',\n",
       " 'analysis',\n",
       " 'till',\n",
       " 'date',\n",
       " ',',\n",
       " 'gci',\n",
       " 'for',\n",
       " 'pathological',\n",
       " 'disorder',\n",
       " 'is',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'laryngeal',\n",
       " '\\\\(',\n",
       " 'glottal',\n",
       " 'source',\n",
       " '\\\\)',\n",
       " 'signal',\n",
       " 'recorded',\n",
       " 'from',\n",
       " 'electroglottograph',\n",
       " ',',\n",
       " 'a',\n",
       " 'dedicated',\n",
       " 'device',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'measure',\n",
       " 'the',\n",
       " 'vocal',\n",
       " 'folds',\n",
       " 'vibration',\n",
       " 'around',\n",
       " 'the',\n",
       " 'larynx',\n",
       " 'we',\n",
       " 'have',\n",
       " 'created',\n",
       " 'a',\n",
       " 'pathological',\n",
       " 'dataset',\n",
       " 'which',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'simultaneous',\n",
       " 'recordings',\n",
       " 'of',\n",
       " 'glottal',\n",
       " 'source',\n",
       " 'and',\n",
       " 'acoustic',\n",
       " 'speech',\n",
       " 'signal',\n",
       " 'of',\n",
       " 'six',\n",
       " 'different',\n",
       " 'disorders',\n",
       " 'from',\n",
       " 'vocal',\n",
       " 'disordered',\n",
       " 'patients',\n",
       " 'the',\n",
       " 'gci',\n",
       " 'locations',\n",
       " 'are',\n",
       " 'manually',\n",
       " 'annotated',\n",
       " 'for',\n",
       " 'disorder',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'we',\n",
       " 'have',\n",
       " 'proposed',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'based',\n",
       " 'gci',\n",
       " 'detection',\n",
       " 'method',\n",
       " 'by',\n",
       " 'fusing',\n",
       " 'deep',\n",
       " 'acoustic',\n",
       " 'speech',\n",
       " 'and',\n",
       " 'linear',\n",
       " 'prediction',\n",
       " 'residual',\n",
       " 'features',\n",
       " 'for',\n",
       " 'robust',\n",
       " 'gci',\n",
       " 'detection',\n",
       " 'the',\n",
       " 'experimental',\n",
       " 'results',\n",
       " 'showed',\n",
       " 'that',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'significantly',\n",
       " 'better',\n",
       " 'than',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'gci',\n",
       " 'detection',\n",
       " 'methods']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = {}  # to remove rare words\n",
    "\n",
    "for doc_content in doc_content_list:\n",
    "    temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b774b30c-4424-4e83-a2ce-e13914d0d7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['construction q constant weight sequences using like approach present scheme constant weight sequences , , given information sequence , construction results sequence specific weight within certain scheme uses design based codes , weight sequences , applications',\n",
       " 'decision study agent population identify supported proposal change , space deliberation process agents , two new proposal , agents identify class spaces , spaces , deliberation process result success deliberation , long supported proposal consider general proposal space deliberation setting study conditions deliberation success analysis dynamic algorithm identify supported proposal',\n",
       " 'decentralized dynamic optimization power network voltage control voltage control power distribution networks devices devices also provide limited power resources used network wide voltage decentralized voltage control strategy voltage mismatch error using gradient \\\\( \\\\) power network , local voltage provide gradient information paper aims analyze performance decentralized based voltage control design two dynamic scenarios \\\\) nodes perform decentralized , \\\\) network time varying voltage control , improve existing convergence voltage based gradient modeling network using process time varying constraints , provide error bound tracking optimal solution error result extended general dynamic optimization problems stochastic processes bounded changes numerical demonstrate results realistic power networks',\n",
       " 'reduction paper , propose procedure given \\\\( \\\\) , language deterministic one b size size bound number , used , obtained',\n",
       " 'rate lower bounds distributed storage one distributed storage system large amount source data long using large number n storage nodes , capacity storage system capacity available , e , 1 n storage nodes time nodes , thus data system average rate n , 1 average node source data , data network nodes average rate , data nodes based data main result , , source data point time must case 2 n provides lower bound average rate data system order source data']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "clean_docs = []\n",
    "for doc_content in doc_content_list:\n",
    "    temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        # word not in stop_words and word_freq[word] >= 5\n",
    "        if word not in stop_words and word_freq[word] >= 5:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    #if doc_str == '':\n",
    "        #doc_str = temp\n",
    "    clean_docs.append(doc_str)\n",
    "clean_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f925923-51d4-4e54-827b-20b9aaf14b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_corpus_str = '\\n'.join(clean_docs)\n",
    "\n",
    "f = open('data/corpus/clean_100.txt', 'w')\n",
    "#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\n",
    "f.write(clean_corpus_str)\n",
    "f.close()\n",
    "\n",
    "#dataset = '20ng'\n",
    "# min_len = 10000\n",
    "# aver_len = 0\n",
    "# max_len = 0 \n",
    "\n",
    "# f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
    "# #f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
    "# lines = f.readlines()\n",
    "# for line in lines:\n",
    "#     line = line.strip()\n",
    "#     temp = line.split()\n",
    "#     aver_len = aver_len + len(temp)\n",
    "#     if len(temp) < min_len:\n",
    "#         min_len = len(temp)\n",
    "#     if len(temp) > max_len:\n",
    "#         max_len = len(temp)\n",
    "# f.close()\n",
    "# aver_len = 1.0 * aver_len / len(lines)\n",
    "# print('min_len : ' + str(min_len))\n",
    "# print('max_len : ' + str(max_len))\n",
    "# print('average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d413a-0c83-41c1-aeee-94c11db6f7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
