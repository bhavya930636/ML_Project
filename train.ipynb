{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672a6b12-48cf-4a65-8c54-2f83d19876d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/sjain/.local/share/jupyter/runtime/kernel-ad2f7367-16af-4255-8d96-790c073e0405.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjain/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16c8e6-b58a-4ae9-b95f-afef565507ce",
   "metadata": {},
   "source": [
    "## # The load_corpus function loads the dataset, including the adjacency matrix (adj), node features (features), labels for training, validation, and testing, and their respective masks. These masks are boolean arrays indicating which nodes belong to which subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9c2859-910c-4c8f-83ee-ab02e43ad642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 300) (90, 27) (0, 300) (0,) (713, 300) (713, 27)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 27 and the array at index 1 has size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m max_degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# import argparse#########\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# parser = argparse.ArgumentParser()##########\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# FLAGS = parser.parse_args()############\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size \u001b[38;5;241m=\u001b[39m load_corpus(dataset)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(adj)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(features)\n",
      "File \u001b[0;32m~/Documents/sem5/ML/Project/ML_Project/utils.py:152\u001b[0m, in \u001b[0;36mload_corpus\u001b[0;34m(dataset_str)\u001b[0m\n\u001b[1;32m    150\u001b[0m     features = sp.vstack((allx, tx)).tolil()\n\u001b[1;32m    151\u001b[0m     if ty.size == 0:#######################\n\u001b[0;32m--> 152\u001b[0m     \t\n\u001b[1;32m    153\u001b[0m \t    print(\"Warning: ty is empty.\")\n\u001b[1;32m    154\u001b[0m \t    labels = ally  # or handle it as you see fit\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 27 and the array at index 1 has size 0"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "import os\n",
    "import random\n",
    "seed = random.randint(1, 200)\n",
    "np.random.seed(seed)\n",
    "# tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "dataset = '100'  # 'citeseer' or 'pubmed' as alternatives\n",
    "model_type = 'gcn'  # 'gcn_cheby' or 'dense' as alternatives\n",
    "learning_rate = 0.02\n",
    "epochs = 200\n",
    "hidden1 = 200\n",
    "dropout = 0.5\n",
    "weight_decay = 0  # 5e-4 can be used as well\n",
    "early_stopping = 10\n",
    "max_degree = 3\n",
    "# import argparse#########\n",
    "\n",
    "# parser = argparse.ArgumentParser()##########\n",
    "# FLAGS = parser.parse_args()############\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
    "print(adj)\n",
    "print(features)\n",
    "print(y_train)\n",
    "print(y_val)\n",
    "print(y_test)\n",
    "print(train_mask)\n",
    "print(val_mask)\n",
    "print(test_mask)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0a564f-0e2d-4cde-a4ae-abdb6ac11c64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39midentity(features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# featureless\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(adj\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features = sp.identity(features.shape[0])  # featureless\n",
    "\n",
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c97b0e-cf41-4915-b69b-bfa2845cfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Set random seed\n",
    "# seed = random.randint(1, 200)\n",
    "# np.random.seed(seed)\n",
    "# tf.set_random_seed(seed)\n",
    "\n",
    "# # Settings\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# flags = tf.app.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "# # 'cora', 'citeseer', 'pubmed'\n",
    "# flags.DEFINE_string('dataset', dataset, 'Dataset string.')\n",
    "# # 'gcn', 'gcn_cheby', 'dense'\n",
    "# flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
    "# flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\n",
    "# flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "# flags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\n",
    "# flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "# flags.DEFINE_float('weight_decay', 0,\n",
    "#                    'Weight for L2 loss on embedding matrix.')  # 5e-4\n",
    "# flags.DEFINE_integer('early_stopping', 10,\n",
    "#                      'Tolerance for early stopping (# of epochs).')\n",
    "# flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# # Load data\n",
    "# adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n",
    "#     FLAGS.dataset)\n",
    "# print(adj)\n",
    "# print(adj[0], adj[1])\n",
    "\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "\n",
    "#TensorFlow Placeholders: Placeholders are created for feeding data into the model. This includes the support matrices (for GCN), input features, labels, and dropout rates.\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    # helper variable for sparse dropout\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "print(features[2][1])\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(\n",
    "        features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(\n",
    "        features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n",
    "                     model.layers[0].embedding], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, pred, labels, duration = evaluate(\n",
    "        features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(\n",
    "              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, pred, labels, test_duration = evaluate(\n",
    "    features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "print(len(test_mask))\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(labels[i])\n",
    "\n",
    "print(\"Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "print(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "print(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7d353-d3b2-4952-9c72-56f5be007a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# doc and word embeddings\n",
    "print('embeddings:')\n",
    "word_embeddings = outs[3][train_size: adj.shape[0] - test_size]\n",
    "train_doc_embeddings = outs[3][:train_size]  # include val docs\n",
    "test_doc_embeddings = outs[3][adj.shape[0] - test_size:]\n",
    "\n",
    "print(len(word_embeddings), len(train_doc_embeddings),\n",
    "      len(test_doc_embeddings))\n",
    "print(word_embeddings)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'r')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "vocab_size = len(words)\n",
    "word_vectors = []\n",
    "for i in range(vocab_size):\n",
    "    word = words[i].strip()\n",
    "    word_vector = word_embeddings[i]\n",
    "    word_vector_str = ' '.join([str(x) for x in word_vector])\n",
    "    word_vectors.append(word + ' ' + word_vector_str)\n",
    "\n",
    "word_embeddings_str = '\\n'.join(word_vectors)\n",
    "f = open('data/' + dataset + '_word_vectors.txt', 'w')\n",
    "f.write(word_embeddings_str)\n",
    "f.close()\n",
    "\n",
    "doc_vectors = []\n",
    "doc_id = 0\n",
    "for i in range(train_size):\n",
    "    doc_vector = train_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "for i in range(test_size):\n",
    "    doc_vector = test_doc_embeddings[i]\n",
    "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n",
    "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_embeddings_str = '\\n'.join(doc_vectors)\n",
    "f = open('data/' + dataset + '_doc_vectors.txt', 'w')\n",
    "f.write(doc_embeddings_str)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
